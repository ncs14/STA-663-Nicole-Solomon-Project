{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Model building and selection has applications across countless fields, including medicine, biology, and life sciences.  The primary interest often is to identify those most strongly predictive of the outcome via model building.  When datasets are abundant in potential covariates there may be concern of admitting or retaining possibly meaningless variables.  To account for and control this \"false\" variable selection, Boos et al (2009) developed the fast false selection rate (FFSR) technique.  This is an algorithm which performs variable selection, model selection, and model-sizing under the context of forward selection. These three aspects are determined by controlling for the 'false selection rate' (FSR) or the rate at which unimportant variables are added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast False Selection Rate Procedure\n",
    "\n",
    "Typical forward selection starts with fitting all covariates to univariate models of the outcome and a single predictor: $Y \\sim X_i,$ $i = 1,\\ldots,k$.  A pre-specified $\\alpha$ level is chosen as the cutoff p-value level for inclusion vs exclusion from the model.  The covariate with the smallest p-value, \"$p$-to-enter\", less than $\\alpha$ is kept and then the process is repeated, with the aforementioned covariate inclusively fixed in future models.  The sequence of p-values for all $k$ covariates is called the \\textit{forward addition sequence}.\n",
    "\n",
    "In the context of forward selection, the FSR algorithm requires a monotonically increasing forward addition sequence.  Hence, if a sequence is not monotone, the sequence it altered by carrying the largest $p$ forward to give a monotonized sequence of p-values: $\\tilde{p}.$  These are then used to compute the FSR ($\\gamma$) level for the model size corresponding to each $\\tilde{p}.$\n",
    "\n",
    "FSR is defined as\n",
    "$$ \\gamma = \\mathrm{E}\\left\\{\\frac{U(Y,X)}{1 + I(Y,X) + U(Y,X)}\\right\\} $$\n",
    "where $U(Y,X)$ and $I(Y,X)$ are the number of uninformative and informative variables in the given model respectively.  Hence, $U + I = S$, the size of the current model.  The expected value is with respect to repeated sampling of the true model, and a 1 is included in the denominator to avoid division by 0 and account for an intercept.  The goal of the FSR procedure is to pre-specify an initial FSR, $\\gamma_0$, and determine the appropriate $\\alpha$-to-enter level to meet this FSR; i.e. what must the cutoff ($\\alpha$) for any variable to be included in the model be, in order to restrict the rate at which unimportant variables enter the model.\n",
    "\n",
    "Now for a given $\\alpha$ the number of uninformative variables in the model is $U(\\alpha) = U(Y,X)$.  This quantity is estimated by $U(\\alpha) \\approx (k - S(\\alpha)) \\hat{\\theta}(\\alpha),$ where $S(\\alpha)$ is the model size at level $\\alpha$, $k$ is the total number of possible predictors, and so $(k - S(\\alpha))$ is an estimate of the number of uninformative variables in the dataset.  $\\hat{\\theta}(\\alpha)$ is the estimated rate at which uninformative variables enter the model.  The original FSR method developed by the same authors estimates $\\hat{\\theta}$ by simulating 'phony' variables (unrelated to the outcome) and computing the rate at which these enter the model.  The new 'fast' method found via simulations that $\\hat{\\theta}(\\alpha) = \\alpha$ is an acceptable substitute and considerably faster.  It was found to be produce more accurate predictions than the former FSR when coupled with a bagging procedure (Boos et al, 2009).  Hence, the fast FSR expression is:\n",
    "$$ \\hat{\\gamma}(\\alpha) = \\frac{(k - S(\\alpha))\\alpha}{1+S(\\alpha)} $$\n",
    "\n",
    "In this manner one can build a table structured as follows:\n",
    "\\begin{center}\n",
    "\\begin{tabular}{l|l|l|l|l|l}\n",
    "\t\\hline\n",
    "\tSize & Variable & p-value & $\\tilde{p}$-value & $\\hat{\\alpha}(\\tilde{p})$ & $\\hat{\\gamma}(\\tilde{p})$ \\\\\n",
    "\t\\hline\n",
    "\t1 & V1 & 1e-05 & 1e-05 & 0.002 & 0.0004 \\\\\n",
    "\t2 & V2 & 0.005 & 0.005 & 0.005 & 0.0120 \\\\\n",
    "\t3 & V3 & 0.021 & 0.021 & 0.017 & 0.2040 \\\\\n",
    "\t4 & V4 & 0.009 & 0.021 & 0.028 & 0.2040 \\\\\n",
    "\t5 & V5 & 0.053 & 0.053 & 0.033 & 0.4241 \\\\\n",
    "\t\\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "\n",
    "where the expression for $\\hat{\\alpha}$ is\n",
    "$$ \\hat{\\alpha}(\\tilde{p}) = \\frac{\\gamma_0(1+S(\\tilde{p}))}{k - S(\\tilde{p})}$$\n",
    "\n",
    "In this manner it is easy to select a model size: the size where $\\hat{\\gamma} < \\gamma_0$.  Alternatively, if one utilizes a model of size $S(\\tilde{p})$ with corresponding $\\hat{\\alpha}(\\tilde{p})$ then one can look to the table to determine what the FSR of the chosen model is.\n",
    "\n",
    "The goal of this project was to implement this algorithm under linear regression in Python. Specifically, functions were be written for the Fast FSR technique for three contexts: in its simplest form for pure variable selection, allowing for forced inclusion of a subset of variables, \n",
    "and with bagging. The code was tested on a \n",
    "dataset available from NCSU in order to demonstrate the technique \n",
    "and its efficiency. The algorithm was also compared to an equivalent R version of the algorithm (Boos, ncsu.edu) on the same dataset to \n",
    "demonstrate the correctness of the Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation & Pseudocode\n",
    "\n",
    "Traditional forward selection can be slow in sizeable datasets.  In order to produce an efficient algorithm, the \n",
    "`regsubsets` function from the `leaps` package in `R` was utilized to conduct the forward selection \n",
    "aspect of the FFSR algorithm.  This function utilizes a branch-and-bound technique to quickly identify optimal subsets in \n",
    "the linear prediction of y by x, rather than fitting all possible models at each iteration (Lumley, 2015).\n",
    "The remainder of the algorithm was implemented in a modular programming fashion in Python.  These modules perform the \n",
    "following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "    1. Perform forward selection via the `regsubsets` function called in Python via the RPY2 library\n",
    "\t2. Store covariate order of entry into the model\n",
    "\t3. Compute p-values for each sequential covariate entering the model\n",
    "    4. Monotonize p-values by taking sequential max\n",
    "\t5. Compute gamma values for each step in model size\n",
    "\t6. Compute alpha values for each monotonized p-value\n",
    "\t7. Compute alpha for a pre-specified gamma (optional)\n",
    "\t8. Estimate parameters for the final fitted model\n",
    "\n",
    "Additional functions were written to neatly compile the results, as well as check for appropriate data input type.\n",
    "All of these functions are called within the primary 'ffsr' function.  A streamlined version of this function was written \n",
    "without the data check or table compilation for the sake of bagging.  An additional 'bag_fsr' function was written for \n",
    "implementation of FFSR with bagging; this function iteratively runs the ffsr algorithm on a duplicate of the original data \n",
    "built from randomly selecting the original rows with replacement.  In this manner the resulting parameter estimates can be \n",
    "averaged, akin to model averaging.  This produces predictions more accurate than those obtained with just one application \n",
    "of the FFSR algorithm (Boos, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing, Profiling, Optimization\n",
    "\n",
    "Unit tests were drafted to assure that functions failed when appropriate, and raise \n",
    "specific warnings.  The functions passed all unit tests as seens below.  The code was also profiled to allow for optimization to improve efficiency and speed.  The primary \n",
    "bottleneck in the primary ffsr function is within the forward selection modular function.  This procedure requires more \n",
    "than 75% of the time necessary to run the functions.  Utilizing an `R` function rather than a Python or C function \n",
    "appears to be the reason for the slow speed.  This issue is addressed in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux2 -- Python 2.7.9 -- py-1.4.25 -- pytest-2.6.3\n",
      "collected 31 items \n",
      "\u001b[0m\n",
      "test_alpha.py ....\n",
      "test_alphag.py ..........\n",
      "test_bagfsr.py ...\n",
      "test_beta.py .....\n",
      "test_covnames.py .\n",
      "test_df_type.py ..\n",
      "test_ffsr.py ..\n",
      "test_gamma.py ...\n",
      "test_pvals.py .\n",
      "\n",
      "\u001b[32m\u001b[1m========================== 31 passed in 2.77 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/bitnami/STA-663-Nicole-Solomon-Project/Tests')\n",
    "!py.test\n",
    "os.chdir('/home/bitnami/STA-663-Nicole-Solomon-Project/Report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application and Comparison\n",
    "\n",
    "The Python algorithm was applied to a NCAA basketball dataset obtained from NCSU (Boos, ncsu.edu).  The R FFSR algorithm was \n",
    "also applied to this data in order to compare the efficiency of the Python algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard FFSR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCSU R Results:\n",
      "\n",
      "Loading required package: leaps\n",
      "    S var   pval  pvmax  alpha      g\n",
      "1   1   2 0.0000 0.0000 0.0056 0.0000\n",
      "2   2   3 0.0001 0.0001 0.0088 0.0004\n",
      "3   5   5 0.0116 0.0116 0.0214 0.0270\n",
      "4   5   4 0.0053 0.0116 0.0214 0.0270\n",
      "5   5   7 0.0025 0.0116 0.0214 0.0270\n",
      "6   6  17 0.0433 0.0433 0.0269 0.0804\n",
      "7   7  15 0.0527 0.0527 0.0333 0.0791\n",
      "8  10   6 0.1056 0.1056 0.0611 0.0864\n",
      "9  10   9 0.0826 0.1056 0.0611 0.0864\n",
      "10 10   8 0.0536 0.1056 0.0611 0.0864\n",
      "11 11  12 0.2350 0.2350 0.0750 0.1566\n",
      "12 12  10 0.2864 0.2864 0.0929 0.1542\n",
      "13 14  13 0.3163 0.3163 0.1500 0.1054\n",
      "14 14  18 0.2697 0.3163 0.1500 0.1054\n",
      "15 15  11 0.4953 0.4953 0.2000 0.1238\n",
      "16 16   1 0.6326 0.6326 0.2833 0.1116\n",
      "17 17  14 0.7056 0.7056 0.4500 0.0784\n",
      "18 18  19 0.8605 0.8605 0.9500 0.0453\n",
      "19 19  16 0.9032 0.9032    Inf 0.0000\n",
      "   user  system elapsed \n",
      "  0.037   0.003   0.040 \n",
      "\n",
      "Python Results:\n",
      "\n",
      "     S  Var       p     p_m alpha_F gamma_F\n",
      "0    1   x2  0.0000  0.0000  0.0056  0.0000\n",
      "1    2   x3  0.0001  0.0001  0.0088  0.0004\n",
      "2    5   x5  0.0116  0.0116  0.0214  0.0270\n",
      "3    5   x4  0.0053  0.0116  0.0214  0.0270\n",
      "4    5   x7  0.0025  0.0116  0.0214  0.0270\n",
      "5    6  x17  0.0433  0.0433  0.0269  0.0804\n",
      "6    7  x15  0.0527  0.0527  0.0333  0.0791\n",
      "7   10   x6  0.1056  0.1056  0.0611  0.0864\n",
      "8   10   x9  0.0826  0.1056  0.0611  0.0864\n",
      "9   10   x8  0.0536  0.1056  0.0611  0.0864\n",
      "10  11  x12  0.2350  0.2350  0.0750  0.1566\n",
      "11  12  x10  0.2864  0.2864  0.0929  0.1542\n",
      "12  14  x13  0.3163  0.3163  0.1500  0.1054\n",
      "13  14  x18  0.2697  0.3163  0.1500  0.1054\n",
      "14  15  x11  0.4953  0.4953  0.2000  0.1238\n",
      "15  16   x1  0.6326  0.6326  0.2833  0.1116\n",
      "16  17  x14  0.7056  0.7056  0.4500  0.0784\n",
      "17  18  x19  0.8605  0.8605  0.9500  0.0453\n",
      "18  19  x16  0.9032  0.9032  1.0000  0.9032\n",
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       0.19 s.\n",
      "  System :       0.02 s.\n",
      "Wall time:       0.21 s.\n"
     ]
    }
   ],
   "source": [
    "%run ffsr_r_run\n",
    "%run -t -m ffsr_p_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFSR function with forced in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCSU R Results:\n",
      "\n",
      "    S var   pval  pvmax  alpha      g\n",
      "1   3  12 0.0000 0.0000 0.0125 0.0000\n",
      "2   3   3 0.0000 0.0000 0.0125 0.0000\n",
      "3   3   5 0.0000 0.0000 0.0125 0.0000\n",
      "4   4   2 0.0000 0.0000 0.0167 0.0000\n",
      "5   6   4 0.0043 0.0043 0.0269 0.0079\n",
      "6   6   7 0.0028 0.0043 0.0269 0.0079\n",
      "7   8  17 0.0539 0.0539 0.0409 0.0659\n",
      "8   8  15 0.0458 0.0539 0.0409 0.0659\n",
      "9  11   6 0.0976 0.0976 0.0750 0.0651\n",
      "10 11   9 0.0962 0.0976 0.0750 0.0651\n",
      "11 11   8 0.0281 0.0976 0.0750 0.0651\n",
      "12 12  10 0.2864 0.2864 0.0929 0.1542\n",
      "13 14  13 0.3163 0.3163 0.1500 0.1054\n",
      "14 14  18 0.2697 0.3163 0.1500 0.1054\n",
      "15 15  11 0.4953 0.4953 0.2000 0.1238\n",
      "16 16   1 0.6326 0.6326 0.2833 0.1116\n",
      "17 17  14 0.7056 0.7056 0.4500 0.0784\n",
      "18 18  19 0.8605 0.8605 0.9500 0.0453\n",
      "19 19  16 0.9032 0.9032    Inf 0.0000\n",
      "   user  system elapsed \n",
      "  0.009   0.001   0.008 \n",
      "\n",
      "Python Results:\n",
      "\n",
      "     S  Var       p     p_m alpha_F gamma_F\n",
      "0    3  x12  0.0000  0.0000  0.0125  0.0000\n",
      "1    3   x3  0.0000  0.0000  0.0125  0.0000\n",
      "2    3   x5  0.0000  0.0000  0.0125  0.0000\n",
      "3    4   x2  0.0000  0.0000  0.0167  0.0000\n",
      "4    6   x4  0.0043  0.0043  0.0269  0.0079\n",
      "5    6   x7  0.0028  0.0043  0.0269  0.0079\n",
      "6    8  x17  0.0539  0.0539  0.0409  0.0659\n",
      "7    8  x15  0.0458  0.0539  0.0409  0.0659\n",
      "8   11   x6  0.0976  0.0976  0.0750  0.0651\n",
      "9   11   x9  0.0962  0.0976  0.0750  0.0651\n",
      "10  11   x8  0.0281  0.0976  0.0750  0.0651\n",
      "11  12  x10  0.2864  0.2864  0.0929  0.1542\n",
      "12  14  x13  0.3163  0.3163  0.1500  0.1054\n",
      "13  14  x18  0.2697  0.3163  0.1500  0.1054\n",
      "14  15  x11  0.4953  0.4953  0.2000  0.1238\n",
      "15  16   x1  0.6326  0.6326  0.2833  0.1116\n",
      "16  17  x14  0.7056  0.7056  0.4500  0.0784\n",
      "17  18  x19  0.8605  0.8605  0.9500  0.0453\n",
      "18  19  x16  0.9032  0.9032  1.0000  0.9032\n",
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       0.04 s.\n",
      "  System :       0.00 s.\n",
      "Wall time:       0.04 s.\n"
     ]
    }
   ],
   "source": [
    "%run ffsr_force_r_run\n",
    "%run -t -m ffsr_force_p_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFSR with bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCSU R Results:\n",
      "\n",
      "   user  system elapsed \n",
      "  0.733   0.010   0.749 \n",
      "\n",
      "[1] \"Mean of estimated alpha-to-enter: 0.0454\"\n",
      "\n",
      "[1] \"Mean size of selected model: 7.69\"\n",
      "\n",
      "Python Results:\n",
      "\n",
      "\n",
      "Mean of estimated alpha-to-enter: 0.0503\n",
      "\n",
      "Mean size of selected model: 7.095\n",
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       6.71 s.\n",
      "  System :       0.27 s.\n",
      "Wall time:       6.01 s.\n"
     ]
    }
   ],
   "source": [
    "%run ffsr_bag_r_run\n",
    "%run -t -m ffsr_bag_p_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the results above, the Python algorithm yields results identical to those returned by the R algorithm.  The bagging results differ somewhat due to the random resampling in Python and R that cannot be matched via setting identical seeds.  These results overall demonstrate the validity of the Python FFSR function.  However, the Python algorithm is significantly slower than the R function.  This is due to the \n",
    "overhead time spent in calling R, running the R function `regsubsets`, and then returning the results for processing.  \n",
    "This procedure would be significantly faster and competitive with the equivalent R algorithm if the Fortran code upon which \n",
    "`regsubsets` is based were to be wrapped in C and thereby made directly callable in Python.  This is a daunting task \n",
    "however as the original Fortran code is of the type Fortran 77 and was written decades ago (Lumley, github.com).  At the very \n",
    "least, an alternative, but performance-wise competitive, forward selection procedure is necessary to improve the Python \n",
    "algorithm beyond its current speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# References\n",
    "\n",
    "1. Boos D, Stefanski L, and Wu Y. \"Fast FSR Variable Selection with Applications to Clinical Trials.\" _Biometrics_. 2009; 65(3): 692-700.\n",
    "2. Boos D and Stefanski L. \"Fast FSR: Controlling the False Selection Rate without Simulation.\" NCSU. http://www4.stat.ncsu.edu/~boos/var.select/fast.fsr.html.\n",
    "3. Thomas Lumley. \"Package 'leaps'\". R Foundation for Statistical Computing, 2015. Version 2.9.\n",
    "4. Thomas Lumley and Alan Miller. \"cran/leaps.\" https://github.com/cran/leaps/tree/master/src.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
