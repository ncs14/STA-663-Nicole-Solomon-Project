\documentclass[12pt]{article}
\usepackage[paper=letterpaper,top=.75in,bottom=.75in,left=1in,right=1in]{geometry}
\usepackage{amsfonts, amsmath, amssymb, amsthm, url, fancyhdr, enumerate, graphicx, float, verbatim}
\usepackage{fontenc,natbib}
\usepackage{color}
\usepackage[english]{babel}
\usepackage{blindtext}
\lhead{Report}
\rhead{First Last}
\chead{}
\lfoot{}
\cfoot{}
\rfoot{\thepage}
\headsep = 10pt
\footskip= 20pt
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\pagestyle{fancy}
\setlength{\parskip}{10pt}
\setlength{\parindent}{0pt}

\newcommand{\al}{\alpha}

\everymath{\displaystyle}


\begin{document}

\title{Implementation of FFSR in Python}
\author{Nicole Solomon}
\maketitle

\vspace{8mm}

\section{Background}

Model building and selection has applications across countless fields, including medicine, biology, and life sciences.  The 
primary interest often is to identify those most strongly predictive of the outcome via model building.  When datasets are 
abundant in potential covariates there may be concern of admitting or retaining possibly meaningless variables.  To account 
for and control this "false" variable selection, Boos et al (2009) developed the fast false selection rate (FFSR) technique.  
This is an algorithm which performs variable selection, model selection, and model-sizing under the context of forward 
selection. These three aspects are determined by controlling for the `false selection rate' (FSR) or the rate at which 
unimportant variables are added to the model.

\subsection{Fast False Selection Rate Procedure}

Typical forward selection starts with fitting all covariates to univariate models of the outcome and a single predictor: $Y 
\sim X_i,$ $i = 1,\ldots,k$.  A pre-specified $\alpha$ level is chosen as the cutoff p-value level for inclusion vs 
exclusion from the model.  The covariate with the smallest p-value, "$p$-to-enter", less than $\alpha$ is kept and then the 
process is repeated, with the aforementioned covariate inclusively fixed in future models.  The sequence of p-values for all 
$k$ covariates is called the \textit{forward addition sequence}.

In the context of forward selection, the FSR algorithm requires a monotonically increasing forward addition sequence.  Hence, 
if a sequence is not monotone, the sequence it altered by carrying the largest $p$ forward to give a monotonized sequence of 
p-values: $\tilde{p}.$  These are then used to compute the FSR ($\gamma$) level for the model size corresponding to each 
$\tilde{p}.$

FSR is defined as
$$ \gamma = \mathrm{E}\left\{\frac{U(Y,X)}{1 + I(Y,X) + U(Y,X)}\right\} $$
where $U(Y,X)$ and $I(Y,X)$ are the number of uninformative and informative variables in the given model respectively.  Hence, 
$U + I = S$, the size of the current model.  The expected value is with respect to repeated sampling of the true model, and a 1 
is included in the denominator to avoid division by 0 and account for an intercept.  The goal of the FSR procedure is to 
pre-specify an initial FSR, $\gamma_0$, and determine the appropriate $\alpha$-to-enter level to meet this FSR; i.e. what must 
the cutoff ($\alpha$) for any variable to be included in the model be, in order to restrict the rate at which unimportant 
variables enter the model.

Now for a given $\alpha$ the number of uninformative variables in the model is $U(\alpha) = U(Y,X)$.  This quantity is estimated
by $U(\alpha) \approx (k - S(\alpha)) \hat{\theta}(\alpha),$ where $S(\alpha)$ is the model size at level $\alpha$, $k$ is the 
total number of possible predictors, and so $(k - S(\alpha))$ is an estimate of the number of uninformative variables in the 
dataset.  $\hat{\theta}(\alpha)$ is the estimated rate at which uninformative variables enter the model.  The original FSR 
method developed by the same authors estimates $\hat{\theta}$ by simulating 'phony' variables (unrelated to the outcome) and 
computing the rate at which these enter the model.  The new 'fast' method found via simulations that $\hat{\theta}(\alpha) = 
\alpha$ is an acceptable substitute and considerably faster.  It was found to be produce more accurate predictions than the 
former FSR when coupled with a bagging procedure (Boos et al, 2009).  Hence, the fast FSR expression is:
$$ \hat{\gamma}(\alpha) = \frac{(k - S(\alpha))\alpha}{1+S(\alpha)} $$

In this manner one can build a table structured as follows:
\begin{center}
\begin{tabular}{l|l|l|l|l|l}
	\hline
	Size & Variable & p-value & $\tilde{p}$-value & $\hat{\alpha}(\tilde{p})$ & $\hat{\gamma}(\tilde{p})$ \\
	\hline
	1 & V1 & 1e-05 & 1e-05 & 0.002 & 0.0004 \\
	2 & V2 & 0.005 & 0.005 & 0.005 & 0.0120 \\
	3 & V3 & 0.021 & 0.021 & 0.017 & 0.2040 \\
	4 & V4 & 0.009 & 0.021 & 0.028 & 0.2040 \\
	5 & V5 & 0.053 & 0.053 & 0.033 & 0.4241 \\
	\hline
\end{tabular}
\end{center}

where the expression for $\hat{\alpha}$ is
$$ \hat{\alpha}(\tilde{p}) = \frac{\gamma_0(1+S(\tilde{p}))}{k - S(\tilde{p})}$$

In this manner it is easy to select a model size: the size where $\hat{\gamma} < \gamma_0$.  Alternatively, if one utilizes 
a model of size $S(\tilde{p})$ with corresponding $\hat{\alpha}(\tilde{p})$ then one can look to the table to determine 
what the FSR of the chosen model is.

The goal of this project was to implement this algorithm under linear regression in Python. Specifically, functions were be 
written for the Fast FSR technique for three contexts: in its simplest form for pure variable selection, allowing for forced 
inclusion of a subset of variables, and with bagging. The code was tested on a 
dataset available from NCSU in order to demonstrate the technique 
and its efficiency. The algorithm was also compared to an equivalent R version of the algorithm (Boos, ncsu.edu) on the same 
dataset to demonstrate the correctness of the Python function.


\section{Implementation}
Traditional forward selection can be slow in datasets with several covariates.  In order to produce an efficient algorithm, the 
\texttt{regsubsets} function from the \texttt{leaps} package in \texttt{R} was utilized to conduct the forward selection 
aspect of the FFSR algorithm.  This function utilizes a branch-and-bound technique to quickly identify optimal subsets in 
the linear prediction of y by x, rather than fitting all possible models at each iteration (Lumley, 2015).

The remainder of the algorithm was implemented in a modular programming fashion in Python.  These modules perform the 
following tasks:
\begin{itemize}
	\item Forward selection via the \texttt{regsubsets} function called in Python via the RPY2 library
	\item Covariate order of entry into the model
	\item P-value computations for each subsequent covariate
	\item Gamma computations for each step in model size
	\item Alpha computation for each monotonized p-value
	\item Alpha computation for a pre-specified gamma
	\item Parameter estimation for the final fitted model
\end{itemize}

Additional functions were written to neatly compile the results, as well as check for appropriate data input type.
All of these functions are called within the primary 'ffsr' function.  A streamlined version of this function was written 
without the data check or table compilation for the sake of bagging.  An additional 'bag\_fsr' function was written for 
implementation of FFSR with bagging; this function iteratively runs the ffsr algorithm on a duplicate of the original data 
built from randomly selecting the original rows with replacement.  In this manner the resulting parameter estimates can be 
averaged, akin to model averaging.  This produces predictions more accurate than those obtained with just one application 
of the FFSR algorithm (Boos, 2009).


\section{Testing, Profiling, Optimization}
Unit tests were drafted to assure that functions failed when appropriate, and raise 
specific warnings.  The code was also profiled to allow for optimization to improve efficiency and speed.  The primary 
bottleneck in the primary ffsr function is within the forward selection modular function.  This procedure requires more 
than 75\% of the time necessary to run the functions.  Utilizing an \texttt{R} function rather than a Python or C function 
appears to be the reason for the slow speed.  This issue is addressed in more detail in Section \ref{sec:comp}.


\section{Application and Comparison}
\label{sec:comp}
The Python algorithm was applied to a NCAA basketball dataset obtained from NCSU (Boos, ncsu.edu).  The R FFSR algorithm was 
also applied to this data in order to compare the efficiency of the Python algorithm.

As seen in the results above, the Python algorithm yields results identical to those returned by the R algorithm.  The bagging 
results differ somewhat due to the random resampling in Python and R that cannot be matched via setting identical seeds.  These 
results overall demonstrate the validity of the Python FFSR function.  However, the Python algorithm is significantly slower 
than the R function.  This is due to the 
overhead time spent in calling R, running the R function \texttt{regsubsets}, and then returning the results for processing.  
This procedure would be significantly faster and competitive with the equivalent R algorithm if the Fortran code upon which 
\texttt{regsubsets} is based were to be wrapped in C and thereby made directly callable in Python.  This is a daunting task 
however as the original Fortran code is of the type Fortran 77 and was written decades ago (Lumley, github.com).  At the very 
least, an alternative, but performance-wise competitive, forward selection procedure is necessary to improve the Python 
algorithm beyond its current speed.


\section*{References}
\begin{enumerate}
    \item Boos D, Stefanski L, and Wu Y. "Fast FSR Variable Selection with Applications to Clinical Trials." \textit{Biometrics}. 
          2009; 65(3): 692-700.
    \item Boos D and Stefanski L. "Fast FSR: Controlling the False Selection Rate without Simulation." NCSU. 
            \texttt{http://www4.stat.ncsu.edu/~boos/var.select/fast.fsr.html}.
    \item Thomas Lumley. "Package 'leaps'". R Foundation for Statistical Computing, 2015. Version 2.9.
    \item Thomas Lumley and Alan Miller. "cran/leaps." \texttt{https://github.com/cran/leaps/tree/}
master/src.
\end{enumerate}

\end{document}