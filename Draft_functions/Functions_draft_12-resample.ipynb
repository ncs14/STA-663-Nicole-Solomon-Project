{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                            \"\"\" Pseudocode for Fast FSR algorithm \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/27/15 \n",
    "    Modified: Improve bagging procedure \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Data type check \"\"\"\n",
    "def df_type(dat):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat = dataset whose type is to be checked / transformed\n",
    "    \n",
    "    ### Output:\n",
    "    #   error msg or True boolean\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if isinstance(dat,pd.DataFrame)==False and isinstance(dat,np.ndarray)==False:\n",
    "        raise Exception(\"Data must be pandas DataFrame\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" p-value computation function \"\"\"\n",
    "def pval_comp(max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of p-values of each covariate at its given entry step\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.stats as st\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    # Pull RSS values & num_obs from fwd_proc object\n",
    "    rss = np.array(ro.r('fwd$rss'))\n",
    "    N = np.array(ro.r('fwd$nn'))\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(rss)-1\n",
    "    \n",
    "    # vector of model sizes\n",
    "    sizes = np.arange(max_size)+1\n",
    "    \n",
    "    # compute the F stats as defined above where p_f - p_r = 1 for each iteration\n",
    "    fstats = (rss[0:max_size] - rss[1:(max_size+1)]) / (rss[1:(max_size+1)] / (N - (sizes+1)))\n",
    "    \n",
    "    # return the p-values by comparing these stats to the F distn: F(1, n - p_f)\n",
    "    return 1 - st.f.cdf(fstats, 1, N-(sizes+1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Covariate model entry order \"\"\"\n",
    "def cov_order(xcolnames,max_size=None,col_incl=None):\n",
    "    \n",
    "    # Input params:\n",
    "    #   xcolnames = array of names of covariates (same order as columns in original dataset)\n",
    "    #   max_size  = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl  = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # array of covariate names sorted according to order of entry into the model\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(xcolnames)\n",
    "        \n",
    "    ### Pull the cov entry order\n",
    "    vorder = ro.r('fwd$vorder[-1]') # remove intercept\n",
    "    vorder = vorder[0:max_size] # keep only the max model size number of covs\n",
    "    \n",
    "    ### Shift these values down by two (one to exclude intercept, one to make python indices)\n",
    "    vorderinds = np.array(vorder)-2\n",
    "    \n",
    "    ### Rearrange the var order st forced vars are at start of list\n",
    "    if col_incl==None:\n",
    "        col_incl = np.arange(max_size)+1\n",
    "    keep = xcolnames[[col_incl-1]] # pull var names of those vars forced into model (this is an array)\n",
    "    poss = [x for x in xcolnames if x not in keep] # pull var names of those not forced in (this is a list)\n",
    "    col_names = np.array(list(keep)+poss) # = rearranged array of varnames w/forced-in vars at start of list\n",
    "    \n",
    "    ### Sort the columns of X in order to obtain the var names in the entry order\n",
    "    return col_names[vorderinds[::]]\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Forward selection function \"\"\"\n",
    "def forward(x,y,max_size=None,col_incl=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # regsubsets R object -- the raw full output of the forward selection proc\n",
    "    \n",
    "    ### Load python packages to call R functions\n",
    "    import rpy2.robjects as ro\n",
    "    import pandas.rpy.common as com\n",
    "    \n",
    "    ### Convert x and y to R matrices <-- MAKE SURE x,y input == DATAFRAMES (or else change them to df's)!!!\n",
    "    ### and declare as R objects in global environment\n",
    "    ro.globalenv['x2'] = com.convert_to_r_matrix(x)\n",
    "    ro.globalenv['y2'] = com.convert_to_r_matrix(y)\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "    ro.globalenv['maxv'] = ro.Vector(max_size)\n",
    "    if col_incl==None:\n",
    "        ro.r('coli=NULL')\n",
    "    else:\n",
    "        ro.globalenv['coli'] = ro.FloatVector(col_incl[:])\n",
    "    \n",
    "    ### Perform forward selection with regsubsets function\n",
    "    ro.globalenv['fwd'] = ro.r('leaps::regsubsets(x=x2,y=y2,method=\"forward\",nvmax=maxv,force.in=coli)')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Gamma computation \"\"\"\n",
    "def gamma_F(pvs, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   pvs      = vector of p-values (monotonically increasing) from forward sel procedure\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of gamma_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # gamma_F_i = p_s_i * (ncov - S_i) / (1 + S_i)\n",
    "    g_F = pvs * (ncov - S) / (1 + S)\n",
    "    \n",
    "    # Check for duplicate p-values\n",
    "    dups = list(set([x for x in list(pvs) if list(pvs).count(x) > 1]))\n",
    "    for i in range(len(dups)): g_F[pvs==dups[i]] = min(g_F[pvs==dups[i]])\n",
    "    \n",
    "    # if table run on all vars, the last gamma = 0,\n",
    "    #  instead set equal to the last pv_mono == final rate of unimp var inclusion\n",
    "    if(g_F[-1]==0): \n",
    "        g_F[-1]=pvs[-1]\n",
    "    \n",
    "    return g_F\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for model selection \"\"\"\n",
    "def alpha_F(g0, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g0       = float pre-specified FSR (gamma0)\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of alpha_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # alpha_F_i = gamma_0 * (1 + S_i) / (ncov - S_i)\n",
    "    alpha_F = g0 * (1 + S) / (ncov - S)\n",
    "    \n",
    "    # if table run on all vars, the last alpha = inf\n",
    "    #  instead set equal to 1 == include all vars\n",
    "    alpha_F[np.isinf(alpha_F)] = 1.\n",
    "    \n",
    "    return alpha_F        \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for specific gamma \"\"\"\n",
    "def alpha_F_g(g, gf, ncov):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g    = float or vector (length k) of specified FSR at which to compute alpha\n",
    "    #   gf   = vector gamma_F's computed from gamma0, pv_sorted\n",
    "    #          used to compute largest size model (S) for which gamma_F < g\n",
    "    #   ncov = integer of total number covariates in data\n",
    "    \n",
    "    ### Output:\n",
    "    # integer alpha_F value\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    ### Compute model size for gf closest to (but still <) g\n",
    "    #S = np.array([max(np.which(x<=y)) for x in gf y in g])+1\n",
    "    if isinstance(g,np.ndarray): # if g is a vector\n",
    "        s_s = [np.where(gf>y) for y in g]\n",
    "        S = np.array([min(x[0]) for x in s_s])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "    else: # if g is a number\n",
    "        S = min(np.where(gf>g)[0])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" Beta-hat computation for specific gamma \"\"\"\n",
    "def beta_est(x, y, g, gf, vname):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x      = python dataframe of original p covariates, n x p\n",
    "    #   y      = python outcome dataframe, n x 1\n",
    "    #   g      = float of specified FSR at which to compute alpha\n",
    "    #   gf     = vector gamma_F's computed from gamma0, pv_mono\n",
    "    #            used to compute largest size model (S) for which gamma_F < g\n",
    "    #   vname  = ordered vector of names of vars entered into model under forward selection\n",
    "    \n",
    "    ### Output:\n",
    "    # array of estimated parameters\n",
    "    \n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    ### Compute model size corresponding to g\n",
    "    S = min(np.where(gf>g)[0])\n",
    "\n",
    "    ### Pull the cov names of those vars included in the above size model\n",
    "    modvars = vname[:S]\n",
    "\n",
    "    ### Fit the linear model using the selected model vars\n",
    "    fit = sm.OLS(y,x.loc[:,list(modvars)]).fit()\n",
    "    betaout = pd.DataFrame([fit.params,fit.bse]).T\n",
    "    betaout.columns = ['beta','beta_se']\n",
    "    \n",
    "    return betaout\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" FSR Results Table \"\"\"\n",
    "def fsrtable(size, vname, p_orig, p_mono, alphaf, gammaf, prec_f='.4f'):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   size   = model size at each step of forward sel proc                   [S]\n",
    "    #   vname  = variable name that entered at each step (num vars = p)        [Var]\n",
    "    #   p_orig = p-values at each step                                         [p]\n",
    "    #   p_mono = ascending p-values                                            [p_s]\n",
    "    #   alphaf = alpha-to-enter (p-value cutoff) for model entry at each step  [alpha_F]\n",
    "    #   gammaf = FSR at each step                                              [gamma_F]\n",
    "    #   prec_f = string of precision (num digits) desired in FSR output table\n",
    "    \n",
    "    ### Output:\n",
    "    # table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Round all arrays\n",
    "    p_od = [format(x,prec_f) for x in p_orig]\n",
    "    p_md = [format(x,prec_f) for x in p_mono]\n",
    "    ad = [format(x,prec_f) for x in alphaf]\n",
    "    gd = [format(x,prec_f) for x in gammaf]\n",
    "    \n",
    "    ### Combine the arrays\n",
    "    tab = pd.DataFrame([size,vname,p_od,p_md,ad,gd]).T\n",
    "    tab.columns = ['S','Var','p','p_m','alpha_F','gamma_F']\n",
    "    \n",
    "    return tab\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR function \"\"\"\n",
    "def ffsr(dat,g0=0.05,betaout=False,gs=None,max_size=None,var_incl=None,bag=False,prec_f='.4f'):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = string of precision (num digits) desired in FSR output table (string to be given to 'format' python fcn)\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    # Table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    #   S:       model size at given step\n",
    "    #   Var:     name of var that entered at given step\n",
    "    #   p:       p-value of var that entered at given step\n",
    "    #   p_m:     mono. inc. p-value (vector or original p-values arranged to be monotonically increasing)\n",
    "    #   alpha_F: cutoff value for model entry given gamma_0 and current p_s value\n",
    "    #   gamma_F: FSR given current alpha_F and model size (== step num)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure dat = pandas dataframes or else convert them\n",
    "    if bag==False:\n",
    "        if df_type(dat)==True:\n",
    "            if isinstance(dat,pd.DataFrame):\n",
    "                d = dat.copy()\n",
    "            else:\n",
    "                if isinstance(dat,np.ndarray):\n",
    "                    d = pd.DataFrame(dat)\n",
    "                    vnum = list(np.arange(d.shape[1])+1)\n",
    "                    vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                    d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "        else:\n",
    "            return df_type(dat)\n",
    "        \n",
    "        ### Remove missing values\n",
    "        d.dropna(inplace=True)\n",
    "        \n",
    "        ### Check that p < n to ensure regression solutions\n",
    "        if (d.shape[1]-1) >= d.shape[0]:\n",
    "            raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    else:\n",
    "        d = dat.copy()\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = d.shape[1]-1\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(d.columns.values[1:], max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, d.shape[1]-1, max_size)\n",
    "    \n",
    "    ### Check if betaout desired, if so compute beta_hat of model corresponding to specific gamma0\n",
    "    if betaout==True or bag==True:\n",
    "        betahats = beta_est(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), g0, g_F, cov_entry_order)\n",
    "        \n",
    "    ### Check if bagging desired\n",
    "    if bag==False: \n",
    "        ### Alpha_F computation for all steps in fwd sel proc\n",
    "        a_F = alpha_F(g0, d.shape[1]-1, max_size)\n",
    "        \n",
    "        ### Model size\n",
    "        S = np.arange(max_size)+1\n",
    "        \n",
    "        ### Combine S, Cov_names, p-vals, sorted p-vals, alpha_F, gamma_F into table\n",
    "        fsr_results = fsrtable(S, cov_entry_order, p_orig, p_mono, a_F, g_F)\n",
    "        \n",
    "        ### Return selected output: FSR table (+ betahat) (+ alpha_specific)\n",
    "        if gs!=None: \n",
    "            ### Compute alpha_F for specific gammas (gs)\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "            else:\n",
    "                return fsr_results, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "        else:\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats\n",
    "            else:\n",
    "                return fsr_results\n",
    "    else:\n",
    "        return betahats, alpha_F_g(g0, g_F, d.shape[1]-1), len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "def bagfsr(dat,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat    = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(dat)==True:\n",
    "        if isinstance(dat,pd.DataFrame):\n",
    "            d = dat.copy()\n",
    "        else:\n",
    "            if isinstance(dat,np.ndarray):\n",
    "                d = pd.DataFrame(dat)\n",
    "                vnum = list(np.arange(d.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(dat)\n",
    "    \n",
    "    ### Remove missing values\n",
    "    d.dropna(inplace=True)\n",
    "    \n",
    "    ### check that p < n to ensure regression solutions\n",
    "    if (d.shape[1]-1) >= d.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(d.shape[1]-1),index=d.columns.values[1:])\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,(d.shape[1]-1)]),columns=d.columns.values[1:])\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "\n",
    "        # Draw with replacement from rows of data\n",
    "        n_row = d.shape[0]\n",
    "        rand_row = np.random.randint(0,n_row,n_row)\n",
    "        newdat = d.iloc[rand_row,:]\n",
    "        newdat.index = np.arange(n_row)+1\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        fsrout = ffsr(newdat,g0,bag=True,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
    "        allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = allses.mean(axis=0)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize\n",
    "    \n",
    "    \n",
    "\n",
    "# Notes: \n",
    "# 1. appropriate transformations are expected to have been applied prior to utilization of FSR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "X = np.random.multivariate_normal(np.zeros(15),np.eye(15),(100))\n",
    "beta = np.array([0,0,5,6,0,0,4,0,0,0,5,0,0,0,0]).reshape(15,1) # signif betas: 3,4,7,11\n",
    "Y = X.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y2 = pd.DataFrame(Y)\n",
    "X2 = pd.DataFrame(X)\n",
    "X2.columns = [\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.concat([Y2,X2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Comparing bagging procedure in python to that in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bagging(dat,B=200):\n",
    "    n_row = dat.shape[0]\n",
    "    for i in xrange(B):\n",
    "            # Draw with replacement from rows of data\n",
    "            rand_row = np.random.randint(0,n_row,n_row)\n",
    "            # rearrange original dataset\n",
    "            newdat = dat.iloc[rand_row,:]\n",
    "            # reassign original indices\n",
    "            newdat.index = np.arange(n_row)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 1: 85.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 -r1 bagging(d)\n",
    "# current bagging process is SLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.132218 s\n",
      "File: <ipython-input-20-ba98bef444db>\n",
      "Function: bagging at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def bagging(dat,B=200):\n",
      "     2         1           21     21.0      0.0      n_row = dat.shape[0]\n",
      "     3       201          297      1.5      0.2      for i in xrange(B):\n",
      "     4                                                       # Draw with replacement from rows of data\n",
      "     5       200         1949      9.7      1.5              rand_row = np.random.randint(0,n_row,n_row)\n",
      "     6                                                       # rearrange original dataset\n",
      "     7       200       116383    581.9     88.0              newdat = dat.iloc[rand_row,:]\n",
      "     8                                                       # reassign original indices\n",
      "     9       200        13568     67.8     10.3              newdat.index = np.arange(n_row)+1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstats = %lprun -r -f bagging bagging(d)\n",
    "lstats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4\n",
      "0   0   1   2   3   4\n",
      "1   5   6   7   8   9\n",
      "2  10  11  12  13  14\n",
      "   0  1  2  3  4\n",
      "0  0  1  2  3  4\n",
      "1  0  1  2  3  4\n",
      "2  0  1  2  3  4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Make sure the sklearn resample function works the way I think it does\n",
    "dummy = pd.DataFrame(np.arange(15).reshape(3,5))\n",
    "print dummy\n",
    "print pd.DataFrame(resample(dummy,replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bagging2(dat,B=200):\n",
    "    newdat = resample(dat, replace=True)\n",
    "    return newdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 73.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit bagging2(d)\n",
    "# much faster than previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user  system elapsed \n",
       "  0.005   0.001   0.006 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i X2,Y2\n",
    "\n",
    "bagging<-function(x,y,B=200){\n",
    "    n<-nrow(x)\n",
    "    for(i in 1:B){ # same procedure as in NCSU algorithm\n",
    "        index<-sample(1:n,n,replace=T)\n",
    "        dat<-cbind(y,x)\n",
    "        }\n",
    "}\n",
    "\n",
    "x2 = as.matrix(X2)\n",
    "y2 = as.matrix(Y2)\n",
    "\n",
    "system.time(bagging(x2,y2))\n",
    "\n",
    "# takes 5ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Redefine bagfsr fcn based on bagging2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bagfsr2(dat,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat    = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(dat)==True:\n",
    "        if isinstance(dat,pd.DataFrame):\n",
    "            d = dat.copy()\n",
    "        else:\n",
    "            if isinstance(dat,np.ndarray):\n",
    "                d = pd.DataFrame(dat)\n",
    "                vnum = list(np.arange(d.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(dat)\n",
    "    \n",
    "    ### Remove missing values\n",
    "    d.dropna(inplace=True)\n",
    "    \n",
    "    ### check that p < n to ensure regression solutions\n",
    "    if (d.shape[1]-1) >= d.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(d.shape[1]-1),index=d.columns.values[1:])\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,(d.shape[1]-1)]),columns=d.columns.values[1:])\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "        # Draw with replacement from rows of data\n",
    "        newdat = pd.DataFrame(resample(d, replace=True))\n",
    "        newdat.columns = d.columns.values\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        fsrout = ffsr(newdat,g0,bag=True,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
    "        allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = allses.mean(axis=0)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            beta       beta_se\n",
      "3   5.000000e+00  9.573162e-16\n",
      "11  5.000000e+00  1.281183e-15\n",
      "4   6.000000e+00  1.187635e-15\n",
      "7   4.000000e+00  1.102468e-15\n",
      "8   2.609024e-15  1.078189e-15\n",
      "13  4.662937e-15  9.165161e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['V5', 'V8', 'V4', 'V12', 'V3', 'V11', 'V2'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: had to relabel the shuffled d2 columns to avoid error on allbetas.loc line\n",
    "\n",
    "fo1 = ffsr(d,g0,bag=True)\n",
    "print fo[0]\n",
    "fo1[0].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "[0 'V1' 'V2' 'V3' 'V4' 'V5' 'V6' 'V7' 'V8' 'V9' 'V10' 'V11' 'V12' 'V13'\n",
      " 'V14' 'V15']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "d2 = pd.DataFrame(resample(d,replace=True))\n",
    "print d2.columns.values # <- resampling resets the column names (b/c it returns an array, not a DF)\n",
    "print d.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 µs, sys: 3 µs, total: 147 µs\n",
      "Wall time: 155 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d2.columns = d.columns.values\n",
    "# compare which column relabeling method is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2 = pd.DataFrame(resample(d,replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 344 µs, sys: 3 µs, total: 347 µs\n",
      "Wall time: 357 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "d2.columns = [a+str(b) for a,b in zip(vchr,list(d2.columns.values))]\n",
    "# longer than the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Compare the old bagfsr to the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.57 s, sys: 76.1 ms, total: 4.65 s\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b1 = bagfsr(d,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.52 s, sys: 74.1 ms, total: 4.59 s\n",
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "b = bagfsr2(d,0.05)\n",
    "# New one is moderately faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Now compare bagfsr2 to NCSU's R function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading required package: leaps\n",
       "   user  system elapsed \n",
       "  0.644   0.021   0.669 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i X2,Y2\n",
    "\n",
    "bag.fsr<-function(x,y,B=100,gam0=.05){\n",
    "# gives average coefficients from fsr.fast6.sim\n",
    "ok<-complete.cases(x,y)\n",
    "x<-x[ok,]                            # get rid of na's\n",
    "y<-y[ok]                             # since regsubsets can't handle na's\n",
    "m<-ncol(x)\n",
    "n<-nrow(x)\n",
    "hold<-matrix(rep(0,m*B),nrow=B)      # holds coefficients\n",
    "interc<-rep(0,B)                     # holds intercepts\n",
    "alphahat<-rep(0,B)                   # holds alphahats\n",
    "size<-rep(0,B)                       # holds sizes\n",
    "for(i in 1:B){\n",
    "index<-sample(1:n,n,replace=T)\n",
    "out<-fsr.fast6.sim(x=x[index,],y=y[index],gam0=gam0)\n",
    "if (out$size>0) hold[i,out$x.ind]<-out$mod$coeff[2:(out$size+1)]\n",
    "interc[i]<-out$mod$coeff[1]\n",
    "alphahat[i]<-out$alphahat.ER\n",
    "size[i]<-out$size\n",
    "}                                    # ends i loop\n",
    "coeff.av<-apply(hold,2, mean)\n",
    "coeff.sd<-rep(0,m)\n",
    "coeff.sd<-sqrt(apply(hold,2, var))\n",
    "interc.av<-mean(interc)\n",
    "interc.sd<-sd(interc)\n",
    "amean<-mean(alphahat)\n",
    "sizem<-mean(size)\n",
    "prop<-rep(0,m)\n",
    "for(j in 1:m){prop[j]<-sum(abs(hold[,j])>0)/B}\n",
    "as.matrix(x)->x                      # in case x is a data frame\n",
    "pred<-x%*%coeff.av+interc.av\n",
    "return(list(coeff.av=coeff.av,coeff.sd=coeff.sd,interc.av=interc.av,pred=pred,\n",
    "            interc.sd=interc.sd,prop=prop,amean=amean,sizem=sizem))\n",
    "}\n",
    "\n",
    "fsr.fast6.sim<-function(x,y,gam0=.05){\n",
    "# estimated alpha for forward selection\n",
    "# short output version\n",
    "require(leaps)\n",
    "ok<-complete.cases(x,y)\n",
    "x<-x[ok,]                            # get rid of na's\n",
    "y<-y[ok]                             # since regsubsets can't handle na's\n",
    "m<-ncol(x)\n",
    "n<-nrow(x)\n",
    "if(m >= n) m1 <- n-5  else m1<-m     # to get rid of NA's in pv\n",
    "vm<-1:m1\n",
    "as.matrix(x)->x                      # in case x is a data frame\n",
    "pvm<-rep(0,m1)                       # to create pvm below\n",
    "regsubsets(x,y,method=\"forward\")->out.x\n",
    "pv.orig<-1-pf((out.x$rss[vm]-out.x$rss[vm+1])*(out.x$nn-(vm+1))/out.x$rss[vm+1],1,out.x$nn-(vm+1))\n",
    "for (i in 1:m1){pvm[i]<-max(pv.orig[1:i])}  # sequential max of pvalues\n",
    "alpha<-c(0,pvm)\n",
    "ng<-length(alpha)\n",
    "S<-rep(0,ng)                         # will contain num. of true entering in orig.\n",
    "real.seq<-data.frame(var=(out.x$vorder-1)[2:(m1+1)],pval=pv.orig,\n",
    "         pvmax=pvm,Rsq=round(1-out.x$rss[2:(m1+1)]/out.x$rss[1],4))\n",
    "for (ia in 2:ng){                    # loop through alpha values for S=size\n",
    "S[ia] <- sum(pvm<=alpha[ia])         # size of models at alpha[ia], S[1]=0\n",
    "}\n",
    "ghat<-(m-S)*alpha/(1+S)              # gammahat_ER\n",
    "# add additional points to make jumps\n",
    "alpha2<-alpha[2:ng]-.0000001\n",
    "ghat2<-(m-S[1:(ng-1)])*alpha2/(1+S[1:(ng-1)])\n",
    "zp<-data.frame(a=c(alpha,alpha2),g=c(ghat,ghat2))\n",
    "zp<-zp[order(zp$a),]\n",
    "gmax<-max(zp$g)\n",
    "index.max<-which.max(zp$g)           # index of largest ghat\n",
    "alphamax<-zp$a[index.max]            # alpha with largest ghat\n",
    "ind<-(ghat <= gam0 & alpha<=alphamax)*1\n",
    "Sind<-S[max(which(ind > 0))]          # model size with ghat just below gam0\n",
    "alphahat.fast<-(1+Sind)*gam0/(m-Sind)  # ER est.\n",
    "size1<-sum(pvm<=alphahat.fast)+1       # size of model including intercept\n",
    "colnames(x)<-colnames(x,do.NULL=F,prefix=\"\")      # corrects for no colnames\n",
    "x<-x[,colnames(x)[(out.x$vorder-1)[2:size1]]]\n",
    "if(size1>1) x.ind<-(out.x$vorder-1)[2:size1]  else x.ind<-0\n",
    "if (size1==1) {mod <- lm(y~1)} else {mod <- lm(y~x)}\n",
    "return(list(mod=mod,size=size1-1,x.ind=x.ind,alphahat.ER=alphahat.fast))\n",
    "}\n",
    "    \n",
    "x2 = as.matrix(X2)\n",
    "y2 = as.matrix(Y2)\n",
    "\n",
    "system.time(bag.fsr(x=x2,y=y2,B=200)->outt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NCSU's is MUCH faster still. \n",
    "#### Check where most time is spent w/line profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 6.99507 s\n",
      "File: <ipython-input-51-33555e0492bd>\n",
      "Function: bagfsr2 at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def bagfsr2(dat,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
      "     2                                               \n",
      "     3                                               ### Input params:\n",
      "     4                                               #   dat    = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
      "     5                                               #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
      "     6                                               #   B      = integer of number of bagged samples\n",
      "     7                                               #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
      "     8                                               #   v_incl = array of cols corresponding to those vars to force into model\n",
      "     9                                               #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
      "    10                                               \n",
      "    11                                               ### Output: \n",
      "    12                                               #   Mean of betahats\n",
      "    13                                               #   SEs of betahats\n",
      "    14                                               #   Avg alpha-to-enter\n",
      "    15                                               #   Avg model size\n",
      "    16                                               #   Prop of times each var included in model\n",
      "    17                                               \n",
      "    18         1            8      8.0      0.0      import numpy as np\n",
      "    19         1            4      4.0      0.0      import pandas as pd\n",
      "    20         1            7      7.0      0.0      from sklearn.utils import resample\n",
      "    21                                               \n",
      "    22                                               ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
      "    23         1            9      9.0      0.0      if df_type(dat)==True:\n",
      "    24         1            2      2.0      0.0          if isinstance(dat,pd.DataFrame):\n",
      "    25         1          447    447.0      0.0              d = dat.copy()\n",
      "    26                                                   else:\n",
      "    27                                                       if isinstance(dat,np.ndarray):\n",
      "    28                                                           d = pd.DataFrame(dat)\n",
      "    29                                                           vnum = list(np.arange(d.shape[1])+1)\n",
      "    30                                                           vchr = list(np.repeat(\"V\",d.shape[1]))\n",
      "    31                                                           d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
      "    32                                               else:\n",
      "    33                                                   return df_type(dat)\n",
      "    34                                               \n",
      "    35                                               ### Remove missing values\n",
      "    36         1         2012   2012.0      0.0      d.dropna(inplace=True)\n",
      "    37                                               \n",
      "    38                                               ### check that p < n to ensure regression solutions\n",
      "    39         1            8      8.0      0.0      if (d.shape[1]-1) >= d.shape[0]:\n",
      "    40                                                   raise Exception(\"N must be > p for valid regression solutions\")\n",
      "    41                                               \n",
      "    42                                               ### Create array to keep track of number of times vars enter model\n",
      "    43         1          331    331.0      0.0      nentries = pd.DataFrame(np.zeros(d.shape[1]-1),index=d.columns.values[1:])\n",
      "    44                                               \n",
      "    45                                               ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
      "    46         1          517    517.0      0.0      allbetas = pd.DataFrame(np.zeros([B,(d.shape[1]-1)]),columns=d.columns.values[1:])\n",
      "    47         1          267    267.0      0.0      allses = allbetas.copy()\n",
      "    48         1            3      3.0      0.0      alphas = []\n",
      "    49         1            1      1.0      0.0      sizes = []\n",
      "    50         1           11     11.0      0.0      np.random.seed(1234)\n",
      "    51                                               \n",
      "    52                                               ### Bagging loops\n",
      "    53       201          577      2.9      0.0      for i in range(B):\n",
      "    54                                                   # Draw with replacement from rows of data\n",
      "    55       200        70725    353.6      1.0          newdat = pd.DataFrame(resample(d, replace=True))\n",
      "    56       200        18398     92.0      0.3          newdat.columns = d.columns.values\n",
      "    57                                                   \n",
      "    58                                                   ### Obtain FSR results\n",
      "    59       200      5982955  29914.8     85.5          fsrout = ffsr(newdat,g0,bag=True,max_size=max_s,var_incl=v_incl)\n",
      "    60       200       217566   1087.8      3.1          allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
      "    61       200       208085   1040.4      3.0          allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
      "    62       200          752      3.8      0.0          alphas.append(fsrout[1])\n",
      "    63       200          439      2.2      0.0          sizes.append(fsrout[2])\n",
      "    64                                           \n",
      "    65                                                   ### Update counts num times var included\n",
      "    66       200       488933   2444.7      7.0          nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
      "    67                                                   \n",
      "    68                                               ### Compute averages\n",
      "    69         1          406    406.0      0.0      avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
      "    70         1          282    282.0      0.0      avgse = allses.mean(axis=0)\n",
      "    71         1           68     68.0      0.0      avgalpha = np.mean(alphas)\n",
      "    72         1           47     47.0      0.0      avgsize = np.mean(sizes)\n",
      "    73         1          268    268.0      0.0      var_props = nentries/float(B)\n",
      "    74         1         1837   1837.0      0.0      cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
      "    75         1          104    104.0      0.0      cov_res.columns = ['betahat','betase','prop_incl']\n",
      "    76                                               \n",
      "    77         1            2      2.0      0.0      return cov_res, avgalpha, avgsize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstats = %lprun -r -f bagfsr2 bagfsr2(d, 0.05)\n",
    "lstats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 85+% of time spent in ffsr function.\n",
    "#### Create new notebook and define superefficient version of ffsr (one that doesn't clean or check data)\n",
    "#### If this still isn't fast enough may need to consider alternative to R's regsubsets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## \n",
    "#### Compare draft 10's ffsr fcn to current (draft 11) ffsr fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 18.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# draft 11\n",
    "%timeit -n100 ffsr(d,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ffsr2_d10 as f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 19.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# draft 10\n",
    "%timeit -n100 f2.ffsr(X2,Y2,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Here we see why bagfsr is so slow: the ffsr proc takes >18ms per iteration --> 200 iterations > 4s long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ffsr4_d12.py\n"
     ]
    }
   ],
   "source": [
    "%%file ffsr4_d12.py\n",
    "\"\"\" Pseudocode for Fast FSR algorithm \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/28/15 \n",
    "    Modified: Improve bagging \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Data type check \"\"\"\n",
    "def df_type(dat):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat = dataset whose type is to be checked / transformed\n",
    "    \n",
    "    ### Output:\n",
    "    #   error msg or True boolean\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if isinstance(dat,pd.DataFrame)==False and isinstance(dat,np.ndarray)==False:\n",
    "        raise Exception(\"Data must be pandas DataFrame\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" p-value computation function \"\"\"\n",
    "def pval_comp(max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of p-values of each covariate at its given entry step\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.stats as st\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    # Pull RSS values & num_obs from fwd_proc object\n",
    "    rss = np.array(ro.r('fwd$rss'))\n",
    "    N = np.array(ro.r('fwd$nn'))\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(rss)-1\n",
    "    \n",
    "    # vector of model sizes\n",
    "    sizes = np.arange(max_size)+1\n",
    "    \n",
    "    # compute the F stats as defined above where p_f - p_r = 1 for each iteration\n",
    "    fstats = (rss[0:max_size] - rss[1:(max_size+1)]) / (rss[1:(max_size+1)] / (N - (sizes+1)))\n",
    "    \n",
    "    # return the p-values by comparing these stats to the F distn: F(1, n - p_f)\n",
    "    return 1 - st.f.cdf(fstats, 1, N-(sizes+1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Covariate model entry order \"\"\"\n",
    "def cov_order(xcolnames,max_size=None,col_incl=None):\n",
    "    \n",
    "    # Input params:\n",
    "    #   xcolnames = array of names of covariates (same order as columns in original dataset)\n",
    "    #   max_size  = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl  = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # array of covariate names sorted according to order of entry into the model\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(xcolnames)\n",
    "        \n",
    "    ### Pull the cov entry order\n",
    "    vorder = ro.r('fwd$vorder[-1]') # remove intercept\n",
    "    vorder = vorder[0:max_size] # keep only the max model size number of covs\n",
    "    \n",
    "    ### Shift these values down by two (one to exclude intercept, one to make python indices)\n",
    "    vorderinds = np.array(vorder)-2\n",
    "    \n",
    "    ### Rearrange the var order st forced vars are at start of list\n",
    "    if col_incl==None:\n",
    "        col_incl = np.arange(max_size)+1\n",
    "    keep = xcolnames[[col_incl-1]] # pull var names of those vars forced into model (this is an array)\n",
    "    poss = [x for x in xcolnames if x not in keep] # pull var names of those not forced in (this is a list)\n",
    "    col_names = np.array(list(keep)+poss) # = rearranged array of varnames w/forced-in vars at start of list\n",
    "    \n",
    "    ### Sort the columns of X in order to obtain the var names in the entry order\n",
    "    return col_names[vorderinds[::]]\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Forward selection function \"\"\"\n",
    "def forward(x,y,max_size=None,col_incl=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # regsubsets R object -- the raw full output of the forward selection proc\n",
    "    \n",
    "    ### Load python packages to call R functions\n",
    "    import rpy2.robjects as ro\n",
    "    import pandas.rpy.common as com\n",
    "    \n",
    "    ### Convert x and y to R matrices <-- MAKE SURE x,y input == DATAFRAMES (or else change them to df's)!!!\n",
    "    ### and declare as R objects in global environment\n",
    "    ro.globalenv['x2'] = com.convert_to_r_matrix(x)\n",
    "    ro.globalenv['y2'] = com.convert_to_r_matrix(y)\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "    ro.globalenv['maxv'] = ro.Vector(max_size)\n",
    "    if col_incl==None:\n",
    "        ro.r('coli=NULL')\n",
    "    else:\n",
    "        ro.globalenv['coli'] = ro.FloatVector(col_incl[:])\n",
    "    \n",
    "    ### Perform forward selection with regsubsets function\n",
    "    ro.globalenv['fwd'] = ro.r('leaps::regsubsets(x=x2,y=y2,method=\"forward\",nvmax=maxv,force.in=coli)')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Gamma computation \"\"\"\n",
    "def gamma_F(pvs, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   pvs      = vector of p-values (monotonically increasing) from forward sel procedure\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of gamma_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # gamma_F_i = p_s_i * (ncov - S_i) / (1 + S_i)\n",
    "    g_F = pvs * (ncov - S) / (1 + S)\n",
    "    \n",
    "    # Check for duplicate p-values\n",
    "    dups = list(set([x for x in list(pvs) if list(pvs).count(x) > 1]))\n",
    "    for i in range(len(dups)): g_F[pvs==dups[i]] = min(g_F[pvs==dups[i]])\n",
    "    \n",
    "    # if table run on all vars, the last gamma = 0,\n",
    "    #  instead set equal to the last pv_mono == final rate of unimp var inclusion\n",
    "    if(g_F[-1]==0): \n",
    "        g_F[-1]=pvs[-1]\n",
    "    \n",
    "    return g_F\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for model selection \"\"\"\n",
    "def alpha_F(g0, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g0       = float pre-specified FSR (gamma0)\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of alpha_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # alpha_F_i = gamma_0 * (1 + S_i) / (ncov - S_i)\n",
    "    alpha_F = g0 * (1 + S) / (ncov - S)\n",
    "    \n",
    "    # if table run on all vars, the last alpha = inf\n",
    "    #  instead set equal to 1 == include all vars\n",
    "    alpha_F[np.isinf(alpha_F)] = 1.\n",
    "    \n",
    "    return alpha_F        \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for specific gamma \"\"\"\n",
    "def alpha_F_g(g, gf, ncov):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g    = float or vector (length k) of specified FSR at which to compute alpha\n",
    "    #   gf   = vector gamma_F's computed from gamma0, pv_sorted\n",
    "    #          used to compute largest size model (S) for which gamma_F < g\n",
    "    #   ncov = integer of total number covariates in data\n",
    "    \n",
    "    ### Output:\n",
    "    # integer alpha_F value\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    ### Compute model size for gf closest to (but still <) g\n",
    "    #S = np.array([max(np.which(x<=y)) for x in gf y in g])+1\n",
    "    if isinstance(g,np.ndarray): # if g is a vector\n",
    "        s_s = [np.where(gf>y) for y in g]\n",
    "        S = np.array([min(x[0]) for x in s_s])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "    else: # if g is a number\n",
    "        S = min(np.where(gf>g)[0])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" Beta-hat computation for specific gamma \"\"\"\n",
    "def beta_est(x, y, g, gf, vname):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x      = python dataframe of original p covariates, n x p\n",
    "    #   y      = python outcome dataframe, n x 1\n",
    "    #   g      = float of specified FSR at which to compute alpha\n",
    "    #   gf     = vector gamma_F's computed from gamma0, pv_mono\n",
    "    #            used to compute largest size model (S) for which gamma_F < g\n",
    "    #   vname  = ordered vector of names of vars entered into model under forward selection\n",
    "    \n",
    "    ### Output:\n",
    "    # array of estimated parameters\n",
    "    \n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    ### Compute model size corresponding to g\n",
    "    S = min(np.where(gf>g)[0])\n",
    "\n",
    "    ### Pull the cov names of those vars included in the above size model\n",
    "    modvars = vname[:S]\n",
    "\n",
    "    ### Fit the linear model using the selected model vars\n",
    "    fit = sm.OLS(y,x.loc[:,list(modvars)]).fit()\n",
    "    betaout = pd.DataFrame([fit.params,fit.bse]).T\n",
    "    betaout.columns = ['beta','beta_se']\n",
    "    \n",
    "    return betaout\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" FSR Results Table \"\"\"\n",
    "def fsrtable(size, vname, p_orig, p_mono, alphaf, gammaf, prec_f='.4f'):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   size   = model size at each step of forward sel proc                   [S]\n",
    "    #   vname  = variable name that entered at each step (num vars = p)        [Var]\n",
    "    #   p_orig = p-values at each step                                         [p]\n",
    "    #   p_mono = ascending p-values                                            [p_s]\n",
    "    #   alphaf = alpha-to-enter (p-value cutoff) for model entry at each step  [alpha_F]\n",
    "    #   gammaf = FSR at each step                                              [gamma_F]\n",
    "    #   prec_f = string of precision (num digits) desired in FSR output table\n",
    "    \n",
    "    ### Output:\n",
    "    # table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Round all arrays\n",
    "    p_od = [format(x,prec_f) for x in p_orig]\n",
    "    p_md = [format(x,prec_f) for x in p_mono]\n",
    "    ad = [format(x,prec_f) for x in alphaf]\n",
    "    gd = [format(x,prec_f) for x in gammaf]\n",
    "    \n",
    "    ### Combine the arrays\n",
    "    tab = pd.DataFrame([size,vname,p_od,p_md,ad,gd]).T\n",
    "    tab.columns = ['S','Var','p','p_m','alpha_F','gamma_F']\n",
    "    \n",
    "    return tab\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR function \"\"\"\n",
    "def ffsr(dat,g0=0.05,betaout=False,gs=None,max_size=None,var_incl=None,bag=False,prec_f='.4f'):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = string of precision (num digits) desired in FSR output table (string to be given to 'format' python fcn)\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    # Table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    #   S:       model size at given step\n",
    "    #   Var:     name of var that entered at given step\n",
    "    #   p:       p-value of var that entered at given step\n",
    "    #   p_m:     mono. inc. p-value (vector or original p-values arranged to be monotonically increasing)\n",
    "    #   alpha_F: cutoff value for model entry given gamma_0 and current p_s value\n",
    "    #   gamma_F: FSR given current alpha_F and model size (== step num)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure dat = pandas dataframes or else convert them\n",
    "    if bag==False:\n",
    "        if df_type(dat)==True:\n",
    "            if isinstance(dat,pd.DataFrame):\n",
    "                d = dat.copy()\n",
    "            else:\n",
    "                if isinstance(dat,np.ndarray):\n",
    "                    d = pd.DataFrame(dat)\n",
    "                    vnum = list(np.arange(d.shape[1])+1)\n",
    "                    vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                    d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "        else:\n",
    "            return df_type(dat)\n",
    "        \n",
    "        ### Remove missing values\n",
    "        d.dropna(inplace=True)\n",
    "        \n",
    "        ### Check that p < n to ensure regression solutions\n",
    "        if (d.shape[1]-1) >= d.shape[0]:\n",
    "            raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    else:\n",
    "        d = dat.copy()\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = d.shape[1]-1\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(d.columns.values[1:], max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, d.shape[1]-1, max_size)\n",
    "    \n",
    "    ### Check if betaout desired, if so compute beta_hat of model corresponding to specific gamma0\n",
    "    if betaout==True or bag==True:\n",
    "        betahats = beta_est(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), g0, g_F, cov_entry_order)\n",
    "        \n",
    "    ### Check if bagging desired\n",
    "    if bag==False: \n",
    "        ### Alpha_F computation for all steps in fwd sel proc\n",
    "        a_F = alpha_F(g0, d.shape[1]-1, max_size)\n",
    "        \n",
    "        ### Model size\n",
    "        S = np.arange(max_size)+1\n",
    "        \n",
    "        ### Combine S, Cov_names, p-vals, sorted p-vals, alpha_F, gamma_F into table\n",
    "        fsr_results = fsrtable(S, cov_entry_order, p_orig, p_mono, a_F, g_F)\n",
    "        \n",
    "        ### Return selected output: FSR table (+ betahat) (+ alpha_specific)\n",
    "        if gs!=None: \n",
    "            ### Compute alpha_F for specific gammas (gs)\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "            else:\n",
    "                return fsr_results, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "        else:\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats\n",
    "            else:\n",
    "                return fsr_results\n",
    "    else:\n",
    "        return betahats, alpha_F_g(g0, g_F, d.shape[1]-1), len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "def bagfsr(dat,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat    = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(dat)==True:\n",
    "        if isinstance(dat,pd.DataFrame):\n",
    "            d = dat.copy()\n",
    "        else:\n",
    "            if isinstance(dat,np.ndarray):\n",
    "                d = pd.DataFrame(dat)\n",
    "                vnum = list(np.arange(d.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(dat)\n",
    "    \n",
    "    ### Remove missing values\n",
    "    d.dropna(inplace=True)\n",
    "    \n",
    "    ### check that p < n to ensure regression solutions\n",
    "    if (d.shape[1]-1) >= d.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(d.shape[1]-1),index=d.columns.values[1:])\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,(d.shape[1]-1)]),columns=d.columns.values[1:])\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "        # Draw with replacement from rows of data\n",
    "        newdat = pd.DataFrame(resample(d, replace=True))\n",
    "        newdat.columns = d.columns.values\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        fsrout = ffsr(newdat,g0,bag=True,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
    "        allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts of num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = allses.mean(axis=0)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize\n",
    "    \n",
    "    \n",
    "\n",
    "# Notes: \n",
    "# 1. appropriate transformations are expected to have been applied prior to utilization of FSR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
