{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                            \"\"\" Pseudocode for Fast FSR algorithm \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/28/15 \n",
    "    Modified: Create super fast FFSR fcn for bagging \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Data type check \"\"\"\n",
    "def df_type(dat):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose: \n",
    "     Check if 'dat' is a pandas Dataframe or a numpy Ndarray\n",
    "    \n",
    "    ### Input params:\n",
    "     dat = dataset whose type is to be checked / transformed\n",
    "    \n",
    "    ### Output:\n",
    "     error msg or True boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if isinstance(dat,pd.DataFrame)==False and isinstance(dat,np.ndarray)==False:\n",
    "        raise Exception(\"Data must be pandas DataFrame\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" p-value computation function \"\"\"\n",
    "def pval_comp(max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of p-values of each covariate at its given entry step\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.stats as st\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    # Pull RSS values & num_obs from fwd_proc object\n",
    "    rss = np.array(ro.r('fwd$rss'))\n",
    "    N = np.array(ro.r('fwd$nn'))\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(rss)-1\n",
    "    \n",
    "    # vector of model sizes\n",
    "    sizes = np.arange(max_size)+1\n",
    "    \n",
    "    # compute the F stats as defined above where p_f - p_r = 1 for each iteration\n",
    "    fstats = (rss[0:max_size] - rss[1:(max_size+1)]) / (rss[1:(max_size+1)] / (N - (sizes+1)))\n",
    "    \n",
    "    # return the p-values by comparing these stats to the F distn: F(1, n - p_f)\n",
    "    return 1 - st.f.cdf(fstats, 1, N-(sizes+1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Covariate model entry order \"\"\"\n",
    "def cov_order(xcolnames,max_size=None,col_incl=None):\n",
    "    \n",
    "    # Input params:\n",
    "    #   xcolnames = array of names of covariates (same order as columns in original dataset)\n",
    "    #   max_size  = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl  = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # array of covariate names sorted according to order of entry into the model\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(xcolnames)\n",
    "        \n",
    "    ### Pull the cov entry order\n",
    "    vorder = ro.r('fwd$vorder[-1]') # remove intercept\n",
    "    vorder = vorder[0:max_size] # keep only the max model size number of covs\n",
    "    \n",
    "    ### Shift these values down by two (one to exclude intercept, one to make python indices)\n",
    "    vorderinds = np.array(vorder)-2\n",
    "    \n",
    "    ### Rearrange the var order st forced vars are at start of list\n",
    "    if col_incl==None:\n",
    "        col_incl = np.arange(max_size)+1\n",
    "    keep = xcolnames[[col_incl-1]] # pull var names of those vars forced into model (this is an array)\n",
    "    poss = [x for x in xcolnames if x not in keep] # pull var names of those not forced in (this is a list)\n",
    "    col_names = np.array(list(keep)+poss) # = rearranged array of varnames w/forced-in vars at start of list\n",
    "    \n",
    "    ### Sort the columns of X in order to obtain the var names in the entry order\n",
    "    return col_names[vorderinds[::]]\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Forward selection function \"\"\"\n",
    "def forward(x,y,max_size=None,col_incl=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # regsubsets R object -- the raw full output of the forward selection proc\n",
    "    \n",
    "    ### Load python packages to call R functions\n",
    "    import rpy2.robjects as ro\n",
    "    import pandas.rpy.common as com\n",
    "    \n",
    "    ### Convert x and y to R matrices <-- MAKE SURE x,y input == DATAFRAMES (or else change them to df's)!!!\n",
    "    ### and declare as R objects in global environment\n",
    "    ro.globalenv['x2'] = com.convert_to_r_matrix(x)\n",
    "    ro.globalenv['y2'] = com.convert_to_r_matrix(y)\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "    ro.globalenv['maxv'] = ro.Vector(max_size)\n",
    "    if col_incl==None:\n",
    "        ro.r('coli=NULL')\n",
    "    else:\n",
    "        ro.globalenv['coli'] = ro.FloatVector(col_incl[:])\n",
    "    \n",
    "    ### Perform forward selection with regsubsets function\n",
    "    ro.globalenv['fwd'] = ro.r('leaps::regsubsets(x=x2,y=y2,method=\"forward\",nvmax=maxv,force.in=coli)')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Gamma computation \"\"\"\n",
    "def gamma_F(pvs, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   pvs      = vector of p-values (monotonically increasing) from forward sel procedure\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of gamma_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # gamma_F_i = p_s_i * (ncov - S_i) / (1 + S_i)\n",
    "    g_F = pvs * (ncov - S) / (1 + S)\n",
    "    \n",
    "    # Check for duplicate p-values\n",
    "    dups = list(set([x for x in list(pvs) if list(pvs).count(x) > 1]))\n",
    "    for i in range(len(dups)): g_F[pvs==dups[i]] = min(g_F[pvs==dups[i]])\n",
    "    \n",
    "    # if table run on all vars, the last gamma = 0,\n",
    "    #  instead set equal to the last pv_mono == final rate of unimp var inclusion\n",
    "    if(g_F[-1]==0): \n",
    "        g_F[-1]=pvs[-1]\n",
    "    \n",
    "    return g_F\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for model selection \"\"\"\n",
    "def alpha_F(g0, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g0       = float pre-specified FSR (gamma0)\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of alpha_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # alpha_F_i = gamma_0 * (1 + S_i) / (ncov - S_i)\n",
    "    alpha_F = g0 * (1 + S) / (ncov - S)\n",
    "    \n",
    "    # if table run on all vars, the last alpha = inf\n",
    "    #  instead set equal to 1 == include all vars\n",
    "    alpha_F[np.isinf(alpha_F)] = 1.\n",
    "    \n",
    "    return alpha_F        \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for specific gamma \"\"\"\n",
    "def alpha_F_g(g, gf, ncov):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g    = float or vector (length k) of specified FSR at which to compute alpha\n",
    "    #   gf   = vector gamma_F's computed from gamma0, pv_sorted\n",
    "    #          used to compute largest size model (S) for which gamma_F < g\n",
    "    #   ncov = integer of total number covariates in data\n",
    "    \n",
    "    ### Output:\n",
    "    # integer alpha_F value\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    ### Compute model size for gf closest to (but still <) g\n",
    "    #S = np.array([max(np.which(x<=y)) for x in gf y in g])+1\n",
    "    if isinstance(g,np.ndarray): # if g is a vector\n",
    "        s_s = [np.where(gf>y) for y in g]\n",
    "        S = np.array([min(x[0]) for x in s_s])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "    else: # if g is a number\n",
    "        S = min(np.where(gf>g)[0])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" Beta-hat computation for specific gamma \"\"\"\n",
    "def beta_est(x, y, g, gf, vname):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x      = python dataframe of original p covariates, n x p\n",
    "    #   y      = python outcome dataframe, n x 1\n",
    "    #   g      = float of specified FSR at which to compute alpha\n",
    "    #   gf     = vector gamma_F's computed from gamma0, pv_mono\n",
    "    #            used to compute largest size model (S) for which gamma_F < g\n",
    "    #   vname  = ordered vector of names of vars entered into model under forward selection\n",
    "    \n",
    "    ### Output:\n",
    "    # array of estimated parameters\n",
    "    \n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    ### Compute model size corresponding to g\n",
    "    S = min(np.where(gf>g)[0])\n",
    "\n",
    "    ### Pull the cov names of those vars included in the above size model\n",
    "    modvars = vname[:S]\n",
    "\n",
    "    ### Fit the linear model using the selected model vars\n",
    "    fit = sm.OLS(y,x.loc[:,list(modvars)]).fit()\n",
    "    betaout = pd.DataFrame([fit.params,fit.bse]).T\n",
    "    betaout.columns = ['beta','beta_se']\n",
    "    \n",
    "    return betaout\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" FSR Results Table \"\"\"\n",
    "def fsrtable(size, vname, p_orig, p_mono, alphaf, gammaf, prec_f='.4f'):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   size   = model size at each step of forward sel proc                   [S]\n",
    "    #   vname  = variable name that entered at each step (num vars = p)        [Var]\n",
    "    #   p_orig = p-values at each step                                         [p]\n",
    "    #   p_mono = ascending p-values                                            [p_s]\n",
    "    #   alphaf = alpha-to-enter (p-value cutoff) for model entry at each step  [alpha_F]\n",
    "    #   gammaf = FSR at each step                                              [gamma_F]\n",
    "    #   prec_f = string of precision (num digits) desired in FSR output table\n",
    "    \n",
    "    ### Output:\n",
    "    # table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Round all arrays\n",
    "    p_od = [format(x,prec_f) for x in p_orig]\n",
    "    p_md = [format(x,prec_f) for x in p_mono]\n",
    "    ad = [format(x,prec_f) for x in alphaf]\n",
    "    gd = [format(x,prec_f) for x in gammaf]\n",
    "    \n",
    "    ### Combine the arrays\n",
    "    tab = pd.DataFrame([size,vname,p_od,p_md,ad,gd]).T\n",
    "    tab.columns = ['S','Var','p','p_m','alpha_F','gamma_F']\n",
    "    \n",
    "    return tab\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR function \"\"\"\n",
    "def ffsr(dat,g0=0.05,betaout=False,gs=None,max_size=None,var_incl=None,bag=False,prec_f='.4f'):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = string of precision (num digits) desired in FSR output table (string to be given to 'format' python fcn)\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    # Table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    #   S:       model size at given step\n",
    "    #   Var:     name of var that entered at given step\n",
    "    #   p:       p-value of var that entered at given step\n",
    "    #   p_m:     mono. inc. p-value (vector or original p-values arranged to be monotonically increasing)\n",
    "    #   alpha_F: cutoff value for model entry given gamma_0 and current p_s value\n",
    "    #   gamma_F: FSR given current alpha_F and model size (== step num)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure dat = pandas dataframes or else convert them\n",
    "    if bag==False:\n",
    "        if df_type(dat)==True:\n",
    "            if isinstance(dat,pd.DataFrame):\n",
    "                d = dat.copy()\n",
    "            else:\n",
    "                if isinstance(dat,np.ndarray):\n",
    "                    d = pd.DataFrame(dat)\n",
    "                    vnum = list(np.arange(d.shape[1])+1)\n",
    "                    vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                    d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "        else:\n",
    "            return df_type(dat)\n",
    "        \n",
    "        ### Remove missing values\n",
    "        d.dropna(inplace=True)\n",
    "        \n",
    "        ### Check that p < n to ensure regression solutions\n",
    "        if (d.shape[1]-1) >= d.shape[0]:\n",
    "            raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    else:\n",
    "        d = dat.copy()\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = d.shape[1]-1\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(d.columns.values[1:], max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, d.shape[1]-1, max_size)\n",
    "    \n",
    "    ### Check if betaout desired, if so compute beta_hat of model corresponding to specific gamma0\n",
    "    if betaout==True or bag==True:\n",
    "        betahats = beta_est(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), g0, g_F, cov_entry_order)\n",
    "        \n",
    "    ### Check if bagging desired\n",
    "    if bag==False: \n",
    "        ### Alpha_F computation for all steps in fwd sel proc\n",
    "        a_F = alpha_F(g0, d.shape[1]-1, max_size)\n",
    "        \n",
    "        ### Model size\n",
    "        S = np.arange(max_size)+1\n",
    "        \n",
    "        ### Combine S, Cov_names, p-vals, sorted p-vals, alpha_F, gamma_F into table\n",
    "        fsr_results = fsrtable(S, cov_entry_order, p_orig, p_mono, a_F, g_F)\n",
    "        \n",
    "        ### Return selected output: FSR table (+ betahat) (+ alpha_specific)\n",
    "        if gs!=None: \n",
    "            ### Compute alpha_F for specific gammas (gs)\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "            else:\n",
    "                return fsr_results, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "        else:\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats\n",
    "            else:\n",
    "                return fsr_results\n",
    "    else:\n",
    "        return betahats, alpha_F_g(g0, g_F, d.shape[1]-1), len(betahats)\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR for bagging function \"\"\"\n",
    "def ffsr_bag(dat,g0=0.05,max_size=None,var_incl=None,prec_f='.4f'):\n",
    "    \n",
    "    ### NOTE: it is assumed that data has been transformed, cleaned, and is given in correct format\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = string of precision (num digits) desired in FSR output table (string to be given to 'format' python fcn)\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   number of param's in final model\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = dat.shape[1]-1\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(dat.iloc[:,1:], pd.DataFrame(dat.iloc[:,0]), max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(dat.columns.values[1:], max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, dat.shape[1]-1, max_size)\n",
    "    \n",
    "    ### Compute beta_hat of model corresponding to specific gamma0\n",
    "    betahats = beta_est(dat.iloc[:,1:], pd.DataFrame(dat.iloc[:,0]), g0, g_F, cov_entry_order)\n",
    "        \n",
    "    return betahats, alpha_F_g(g0, g_F, dat.shape[1]-1), len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "def bagfsr(dat,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat    = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(dat)==True:\n",
    "        if isinstance(dat,pd.DataFrame):\n",
    "            d = dat.copy()\n",
    "        else:\n",
    "            if isinstance(dat,np.ndarray):\n",
    "                d = pd.DataFrame(dat)\n",
    "                vnum = list(np.arange(d.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(dat)\n",
    "    \n",
    "    ### Remove missing values\n",
    "    d.dropna(inplace=True)\n",
    "    \n",
    "    ### check that p < n to ensure regression solutions\n",
    "    if (d.shape[1]-1) >= d.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(d.shape[1]-1),index=d.columns.values[1:])\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,(d.shape[1]-1)]),columns=d.columns.values[1:])\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "        # Draw with replacement from rows of data\n",
    "        newdat = pd.DataFrame(resample(d, replace=True))\n",
    "        newdat.columns = d.columns.values\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        fsrout = ffsr_bag(newdat,g0,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
    "        allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts of num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = allses.mean(axis=0)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize\n",
    "    \n",
    "    \n",
    "\n",
    "# Notes: \n",
    "# 1. appropriate transformations are expected to have been applied prior to utilization of FSR algorithm\n",
    "\n",
    "\"\"\" Draft 13 module defined at bottom of notebook \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "X = np.random.multivariate_normal(np.zeros(15),np.eye(15),(100))\n",
    "beta = np.array([0,0,5,6,0,0,4,0,0,0,5,0,0,0,0]).reshape(15,1) # signif betas: 3,4,7,11\n",
    "Y = X.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y2 = pd.DataFrame(Y)\n",
    "X2 = pd.DataFrame(X)\n",
    "X2.columns = [\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.concat([Y2,X2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 26.17 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 19.4 ms per loop\n",
      "The slowest run took 7.13 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100 loops, best of 3: 18.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "##### Check speed of ffsr_bag vs ffsr\n",
    "%timeit ffsr(d,0.05)\n",
    "%timeit ffsr_bag(d,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.037304 s\n",
      "File: <ipython-input-1-0eacd6692129>\n",
      "Function: ffsr_bag at line 391\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   391                                           def ffsr_bag(dat,g0=0.05,max_size=None,var_incl=None,prec_f='.4f'):\n",
      "   392                                               \n",
      "   393                                               ### NOTE: it is assumed that data has been transformed, cleaned, and is given in correct format\n",
      "   394                                               \n",
      "   395                                               ### Input params:\n",
      "   396                                               #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
      "   397                                               #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
      "   398                                               #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
      "   399                                               #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
      "   400                                               #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
      "   401                                               #   var_incl = array of cols corresponding to those vars to force into model\n",
      "   402                                               #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
      "   403                                               #   prec_f   = string of precision (num digits) desired in FSR output table (string to be given to 'format' python fcn)\n",
      "   404                                               \n",
      "   405                                               ### Output: \n",
      "   406                                               #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
      "   407                                               #   vector of estimated beta param's for final model (based on g0)\n",
      "   408                                               #       and\n",
      "   409                                               #   vector of alpha_F's for specified gamma's (g)\n",
      "   410                                               #       and\n",
      "   411                                               #   number of param's in final model\n",
      "   412                                           \n",
      "   413         1            6      6.0      0.0      import numpy as np\n",
      "   414         1            3      3.0      0.0      import pandas as pd\n",
      "   415                                               \n",
      "   416                                               ### If max model size not specified, select all possible cov.s\n",
      "   417         1            2      2.0      0.0      if max_size==None:\n",
      "   418         1           20     20.0      0.1          max_size = dat.shape[1]-1\n",
      "   419                                                   \n",
      "   420                                               ### Perform forward selection\n",
      "   421         1        28922  28922.0     77.5      fwd_sel = forward(dat.iloc[:,1:], pd.DataFrame(dat.iloc[:,0]), max_size, var_incl)\n",
      "   422                                               \n",
      "   423                                               ### Save order of covariate entry into model\n",
      "   424         1          636    636.0      1.7      cov_entry_order = cov_order(dat.columns.values[1:], max_size, var_incl)\n",
      "   425                                               \n",
      "   426                                               ### Compute p-value of each covariate entering the model\n",
      "   427         1         1277   1277.0      3.4      p_orig = pval_comp(max_size)\n",
      "   428                                               \n",
      "   429                                               ### Arrange p-values in mono. inc. order\n",
      "   430        16           94      5.9      0.3      p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
      "   431                                                   \n",
      "   432                                               ### Gamma_F computation\n",
      "   433         1          145    145.0      0.4      g_F = gamma_F(p_mono, dat.shape[1]-1, max_size)\n",
      "   434                                               \n",
      "   435                                               ### Compute beta_hat of model corresponding to specific gamma0\n",
      "   436         1         6149   6149.0     16.5      betahats = beta_est(dat.iloc[:,1:], pd.DataFrame(dat.iloc[:,0]), g0, g_F, cov_entry_order)\n",
      "   437                                                   \n",
      "   438         1           50     50.0      0.1      return betahats, alpha_F_g(g0, g_F, d.shape[1]-1), len(betahats)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstats = %lprun -r -f ffsr_bag ffsr_bag(d,0.05)\n",
    "lstats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### The time suck occurs primarily in the forward function (need faster alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Could try defining a forward selection proc in Python\n",
    "# but would this really speed up the code? Not with large (p) datasets since this still steps through all p cov's\n",
    "\n",
    "# Need to translate the Fortran Leaps functions which rely on the 'efficient' branch-and-bound algorithm in order to perform\n",
    "# forward selection\n",
    "\n",
    "def seq_forw_select(features, max_k, criterion_func, print_steps=False):\n",
    "    \"\"\"\n",
    "    Implementation of a Sequential Forward Selection algorithm.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        features (list): The feature space as a list of features.\n",
    "        max_k: Termination criterion; the size of the returned feature subset.\n",
    "        criterion_func (function): Function that is used to evaluate the\n",
    "            performance of the feature subset.\n",
    "        print_steps (bool): Prints the algorithm procedure if True.\n",
    "    \n",
    "    Returns the selected feature subset, a list of features of length max_k.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialization\n",
    "    feat_sub = [features[0]]\n",
    "    k = 0\n",
    "    d = len(features)\n",
    "    if max_k > d:\n",
    "        max_k = d\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Inclusion step\n",
    "        if print_steps:\n",
    "            print('\\nInclusion from feature space', features)\n",
    "        crit_func_max = criterion_func(feat_sub + [features[1]])\n",
    "        best_feat = features[1]\n",
    "        for x in features[2:]:\n",
    "            crit_func_eval = criterion_func(feat_sub + [x])\n",
    "            if crit_func_eval < crit_func_max:\n",
    "                crit_func_max = crit_func_eval\n",
    "                best_feat = x\n",
    "        feat_sub.append(best_feat)\n",
    "        if print_steps:\n",
    "            print('include: {} -> feature subset: {}'.format(best_feat, feat_sub))\n",
    "        features.remove(best_feat)\n",
    "        \n",
    "        # Termination condition\n",
    "        k = len(feat_sub)\n",
    "        if k == max_k:\n",
    "            break\n",
    "                \n",
    "    return feat_sub\n",
    "\n",
    "\n",
    "\n",
    "def criterion_f(f):\n",
    "\n",
    "    x = X2.copy()\n",
    "    x.insert(0,'int',1.)\n",
    "    y = Y2\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    mod = sm.OLS(y, x.loc[:,f])\n",
    "    rs = mod.fit()\n",
    "    \n",
    "    return rs.pvalues[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feats = list(X2)\n",
    "\n",
    "# X3 = X2.copy()\n",
    "# X3.insert(0,'int',1.)\n",
    "\n",
    "# b = ['int'] + [feats[3]]\n",
    "# mod = sm.OLS(Y2, X3.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# print rs.pvalues[-1]\n",
    "# print criterion_f(b)\n",
    "\n",
    "# b = b + [feats[2]]\n",
    "# mod = sm.OLS(Y2, X2.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# print rs.pvalues\n",
    "# print criterion_f(b)\n",
    "\n",
    "# b = b + [feats[6]]\n",
    "# mod = sm.OLS(Y2, X3.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# print rs.pvalues\n",
    "# print criterion_f(b)\n",
    "\n",
    "# b = b + [feats[2]]\n",
    "# mod = sm.OLS(Y2, X3.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# print rs.pvalues\n",
    "# print criterion_f(b)\n",
    "\n",
    "# b = b + [feats[10]]\n",
    "# mod = sm.OLS(Y2, X3.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# print rs.pvalues\n",
    "# print criterion_f(b)\n",
    "\n",
    "# b = b + [feats[1]]\n",
    "# mod = sm.OLS(Y2, X3.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# print rs.pvalues\n",
    "# print criterion_f(b)\n",
    "\n",
    "# feats = list(X3)\n",
    "# print feats\n",
    "# seq_forw_select(feats, 6, criterion_f, print_steps=True)\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "# mod = sm.OLS(Y2, X2.loc[:,b])\n",
    "# rs = mod.fit()\n",
    "# rs.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Attempt at using cython_gsl in order to call linear fitting fcn from GNU Scientific Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: /home/bitnami/.cache/ipython/cython/_cython_magic_c6c0298838cc0b4d4a23302d59242765.pyx:8:17: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n",
      "warning: /home/bitnami/.cache/ipython/cython/_cython_magic_c6c0298838cc0b4d4a23302d59242765.pyx:8:23: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n",
      "warning: /home/bitnami/.cache/ipython/cython/_cython_magic_c6c0298838cc0b4d4a23302d59242765.pyx:8:29: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n"
     ]
    },
    {
     "ename": "CompileError",
     "evalue": "command 'gcc' failed with exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0f0cff763e83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'cython'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'-l gsl -l gslcblas'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\ncimport cython\\nfrom cython_gsl cimport *\\n\\ndef main ():\\n    cdef int i, n\\n    n = 4\\n    cdef double x[4], y[4], w[4]\\n    x[0] =  1970\\n    x[1] = 1980\\n    x[2] = 1990\\n    x[3] = 2000\\n    y[0] = 12\\n    y[1] = 11\\n    y[2] = 14\\n    y[3] = 13\\n    w[0] = 0.1\\n    w[1] = 0.2\\n    w[2] = 0.3\\n    w[3] = 0.4\\n\\n    cdef double c0, c1, cov00, cov01, cov11, chisq\\n\\n    gsl_fit_wlinear (x, 1, w, 1, y, 1, n,\\n                     &c0, &c1, &cov00, &cov01, &cov11,\\n                     &chisq)\\n\\n    print \"# best fit: Y = %g + %g X\\\\n\" % (c0, c1)\\n    print \"# covariance matrix:\\\\n\"\\n    print \"# [ %g, %g\\\\n#   %g, %g]\\\\n\" % (cov00, cov01, cov01, cov11)\\n    print \"# chisq = %g\\\\n\", chisq\\n\\n    for i from 0 <= i < n:\\n        print \"data: %g %g %g\\\\n\" % ( x[i], y[i], 1/sqrt(w[i]))\\n\\n    print \"\\\\n\"\\n\\n    cdef double xf, yf, yf_err\\n    for i from -30 <= i < 130:\\n        xf = x[0] + (i/100.0) * (x[n-1] - x[0])\\n\\n        gsl_fit_linear_est (xf, c0, c1, cov00, cov01, cov11, &yf, &yf_err)\\n\\n        print \"fit: %g %g\\\\n\" %(xf, yf)\\n        print \"hi : %g %g\\\\n\" %(xf, yf + yf_err)\\n        print \"lo : %g %g\\\\n\" %(xf, yf - yf_err)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2264\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2265\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/Cython/Build/IpythonMagic.pyc\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/Cython/Build/IpythonMagic.pyc\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyx_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_lib\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlib_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_code_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/command/build_ext.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_extensions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/command/build_ext.pyc\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/command/build_ext.pyc\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    494\u001b[0m                                          \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                                          \u001b[0mextra_postargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                                          depends=ext.depends)\n\u001b[0m\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# XXX -- this is a Vile HACK!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/ccompiler.pyc\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_postargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;31m# Return *all* object filenames, not just the ones we just built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/unixccompiler.pyc\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(self, obj, src, ext, cc_args, extra_postargs, pp_opts)\u001b[0m\n\u001b[0;32m    120\u001b[0m                        extra_postargs)\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsExecError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCompileError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     def create_static_lib(self, objects, output_libname,\n",
      "\u001b[1;31mCompileError\u001b[0m: command 'gcc' failed with exit status 1"
     ]
    }
   ],
   "source": [
    "%%cython -l gsl -l gslcblas\n",
    "\n",
    "cimport cython\n",
    "from cython_gsl cimport *\n",
    "\n",
    "def main ():\n",
    "    cdef int i, n\n",
    "    n = 4\n",
    "    cdef double x[4], y[4], w[4]\n",
    "    x[0] =  1970\n",
    "    x[1] = 1980\n",
    "    x[2] = 1990\n",
    "    x[3] = 2000\n",
    "    y[0] = 12\n",
    "    y[1] = 11\n",
    "    y[2] = 14\n",
    "    y[3] = 13\n",
    "    w[0] = 0.1\n",
    "    w[1] = 0.2\n",
    "    w[2] = 0.3\n",
    "    w[3] = 0.4\n",
    "\n",
    "    cdef double c0, c1, cov00, cov01, cov11, chisq\n",
    "\n",
    "    gsl_fit_wlinear (x, 1, w, 1, y, 1, n,\n",
    "                     &c0, &c1, &cov00, &cov01, &cov11,\n",
    "                     &chisq)\n",
    "\n",
    "    print \"# best fit: Y = %g + %g X\\n\" % (c0, c1)\n",
    "    print \"# covariance matrix:\\n\"\n",
    "    print \"# [ %g, %g\\n#   %g, %g]\\n\" % (cov00, cov01, cov01, cov11)\n",
    "    print \"# chisq = %g\\n\", chisq\n",
    "\n",
    "    for i from 0 <= i < n:\n",
    "        print \"data: %g %g %g\\n\" % ( x[i], y[i], 1/sqrt(w[i]))\n",
    "\n",
    "    print \"\\n\"\n",
    "\n",
    "    cdef double xf, yf, yf_err\n",
    "    for i from -30 <= i < 130:\n",
    "        xf = x[0] + (i/100.0) * (x[n-1] - x[0])\n",
    "\n",
    "        gsl_fit_linear_est (xf, c0, c1, cov00, cov01, cov11, &yf, &yf_err)\n",
    "\n",
    "        print \"fit: %g %g\\n\" %(xf, yf)\n",
    "        print \"hi : %g %g\\n\" %(xf, yf + yf_err)\n",
    "        print \"lo : %g %g\\n\" %(xf, yf - yf_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CompileError",
     "evalue": "command 'gcc' failed with exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-78a1d24dec9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'cython'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'-lgsl -lgslcblas'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu\"'''\\nGibbs sampler for function:\\n\\nf(x,y) = x x^2 \\\\exp(-xy^2 - y^2 + 2y - 4x)\\n\\nusing conditional distributions:\\n\\nx|y \\\\sim Gamma(3, y^2 +4)\\ny|x \\\\sim Normal(\\\\frac{1}{1+x}, \\\\frac{1}{2(1+x)})\\n\\nOriginal version written by Flavio Coelho.\\nTweaked by Chris Fonnesbeck.\\nPorted to CythonGSL Thomas V. Wiecki.\\n'''\\ncimport cython\\nfrom cython_gsl cimport *\\n\\nimport numpy as np\\ncimport numpy as np\\n\\nfrom libc.math cimport sqrt\\n\\ncdef gsl_rng *r = gsl_rng_alloc(gsl_rng_mt19937)\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef gibbs(int N=20000, int thin=500):\\n    cdef: \\n        double x = 0\\n        double y = 0\\n        Py_ssize_t i, j\\n        np.ndarray[np.float64_t, ndim=2] samples = np.empty((N, 2), dtype=np.float64)\\n\\n    for i in range(N):\\n        for j in range(thin):\\n            x = gsl_ran_gamma(r, 3, 1.0 / (y * y + 4))\\n            y = gsl_ran_gaussian(r, 1.0 / sqrt(x + 1))\\n        samples[i, 0] = x\\n        samples[i, 1] = y\\n    return samples\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2264\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2265\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/Cython/Build/IpythonMagic.pyc\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/site-packages/Cython/Build/IpythonMagic.pyc\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyx_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_lib\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlib_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_code_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/command/build_ext.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_extensions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/command/build_ext.pyc\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/command/build_ext.pyc\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    494\u001b[0m                                          \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                                          \u001b[0mextra_postargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                                          depends=ext.depends)\n\u001b[0m\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# XXX -- this is a Vile HACK!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/ccompiler.pyc\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_postargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;31m# Return *all* object filenames, not just the ones we just built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bitnami/anaconda/lib/python2.7/distutils/unixccompiler.pyc\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(self, obj, src, ext, cc_args, extra_postargs, pp_opts)\u001b[0m\n\u001b[0;32m    120\u001b[0m                        extra_postargs)\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsExecError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCompileError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     def create_static_lib(self, objects, output_libname,\n",
      "\u001b[1;31mCompileError\u001b[0m: command 'gcc' failed with exit status 1"
     ]
    }
   ],
   "source": [
    "%%cython -lgsl -lgslcblas\n",
    "'''\n",
    "Gibbs sampler for function:\n",
    "\n",
    "f(x,y) = x x^2 \\exp(-xy^2 - y^2 + 2y - 4x)\n",
    "\n",
    "using conditional distributions:\n",
    "\n",
    "x|y \\sim Gamma(3, y^2 +4)\n",
    "y|x \\sim Normal(\\frac{1}{1+x}, \\frac{1}{2(1+x)})\n",
    "\n",
    "Original version written by Flavio Coelho.\n",
    "Tweaked by Chris Fonnesbeck.\n",
    "Ported to CythonGSL Thomas V. Wiecki.\n",
    "'''\n",
    "cimport cython\n",
    "from cython_gsl cimport *\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "cdef gsl_rng *r = gsl_rng_alloc(gsl_rng_mt19937)\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "def gibbs(int N=20000, int thin=500):\n",
    "    cdef: \n",
    "        double x = 0\n",
    "        double y = 0\n",
    "        Py_ssize_t i, j\n",
    "        np.ndarray[np.float64_t, ndim=2] samples = np.empty((N, 2), dtype=np.float64)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(thin):\n",
    "            x = gsl_ran_gamma(r, 3, 1.0 / (y * y + 4))\n",
    "            y = gsl_ran_gaussian(r, 1.0 / sqrt(x + 1))\n",
    "        samples[i, 0] = x\n",
    "        samples[i, 1] = y\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ffsr5_d13.py\n"
     ]
    }
   ],
   "source": [
    "%%file ffsr5_d13.py\n",
    "\"\"\" Pseudocode for Fast FSR algorithm \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/28/15 \n",
    "    Modified: Create super fast FFSR fcn for bagging \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Data type check \"\"\"\n",
    "def df_type(dat):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose: \n",
    "    # Check if 'dat' is a pandas Dataframe or a numpy Ndarray\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat = dataset whose type is to be checked / transformed\n",
    "    \n",
    "    ### Output:\n",
    "    # error msg or True boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if isinstance(dat,pd.DataFrame)==False and isinstance(dat,np.ndarray)==False:\n",
    "        raise Exception(\"Data must be pandas DataFrame\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" p-value computation function \"\"\"\n",
    "def pval_comp(max_size=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "     Compute the sequential p-values of the variables added to the model\n",
    "    \n",
    "    ### Input params:\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of p-values of each covariate at its given entry step\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object \n",
    "    ###       (requires running 'forward' fcn prior to this fcn)\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.stats as st\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    # Pull RSS values & num_obs from fwd_proc object\n",
    "    rss = np.array(ro.r('fwd$rss'))\n",
    "    N = np.array(ro.r('fwd$nn'))\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(rss)-1\n",
    "    \n",
    "    # vector of model sizes\n",
    "    sizes = np.arange(max_size)+1\n",
    "    \n",
    "    # compute the F stats as defined above where p_f - p_r = 1 for each iteration\n",
    "    fstats = (rss[0:max_size] - rss[1:(max_size+1)]) / (rss[1:(max_size+1)] / (N - (sizes+1)))\n",
    "    \n",
    "    # return the p-values by comparing these stats to the F distn: F(1, n - p_f)\n",
    "    return 1 - st.f.cdf(fstats, 1, N-(sizes+1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Covariate model entry order \"\"\"\n",
    "def cov_order(xcolnames,max_size=None,col_incl=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose: \n",
    "    # Determine order of covariate entry into final model\n",
    "    \n",
    "    ### Input params:\n",
    "    # xcolnames = array of names of covariates (same order as columns in original dataset)\n",
    "    # max_size  = integer max no. of vars in final model (largest model size desired)\n",
    "    # col_incl  = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # array of covariate names sorted according to order of entry into the model\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object \n",
    "    ###       (requires running 'forward' fcn prior to this fcn)\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(xcolnames)\n",
    "        \n",
    "    ### Pull the cov entry order\n",
    "    vorder = ro.r('fwd$vorder[-1]') # remove intercept\n",
    "    vorder = vorder[0:max_size] # keep only the max model size number of covs\n",
    "    \n",
    "    ### Shift these values down by two (one to exclude intercept, one to make python indices)\n",
    "    vorderinds = np.array(vorder)-2\n",
    "    \n",
    "    ### Rearrange the var order st forced vars are at start of list\n",
    "    if col_incl==None:\n",
    "        col_incl = np.arange(max_size)+1\n",
    "    keep = xcolnames[[col_incl-1]] # pull var names of those vars forced into model (this is an array)\n",
    "    poss = [x for x in xcolnames if x not in keep] # pull var names of those not forced in (this is a list)\n",
    "    col_names = np.array(list(keep)+poss) # = rearranged array of varnames w/forced-in vars at start of list\n",
    "    \n",
    "    ### Sort the columns of X in order to obtain the var names in the entry order\n",
    "    return col_names[vorderinds[::]]\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Forward selection function \"\"\"\n",
    "def forward(x,y,max_size=None,col_incl=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "    # Perform the forward selection procedure via the R function leaps::regsubsets\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # regsubsets R object -- the raw full output of the forward selection proc\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Load python packages to call R functions\n",
    "    import rpy2.robjects as ro\n",
    "    import pandas.rpy.common as com\n",
    "    \n",
    "    ### Convert x and y to R matrices <-- MAKE SURE x,y input == DATAFRAMES (or else change them to df's)!!!\n",
    "    ### and declare as R objects in global environment\n",
    "    ro.globalenv['x2'] = com.convert_to_r_matrix(x)\n",
    "    ro.globalenv['y2'] = com.convert_to_r_matrix(y)\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "    ro.globalenv['maxv'] = ro.Vector(max_size)\n",
    "    if col_incl==None:\n",
    "        ro.r('coli=NULL')\n",
    "    else:\n",
    "        ro.globalenv['coli'] = ro.FloatVector(col_incl[:])\n",
    "    \n",
    "    ### Perform forward selection with regsubsets function\n",
    "    ro.globalenv['fwd'] = ro.r('leaps::regsubsets(x=x2,y=y2,method=\"forward\",nvmax=maxv,force.in=coli)')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Gamma computation \"\"\"\n",
    "def gamma_F(pvs, ncov, max_size=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "    # Compute the gamma (FSR) values at each step in the model build procedure\n",
    "     \n",
    "    ### Input params:\n",
    "    #   pvs      = vector of p-values (monotonically increasing) from forward sel procedure\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of gamma_F values\n",
    "    \"\"\"    \n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # gamma_F_i = p_s_i * (ncov - S_i) / (1 + S_i)\n",
    "    g_F = pvs * (ncov - S) / (1 + S)\n",
    "    \n",
    "    # Check for duplicate p-values\n",
    "    dups = list(set([x for x in list(pvs) if list(pvs).count(x) > 1]))\n",
    "    for i in range(len(dups)): g_F[pvs==dups[i]] = min(g_F[pvs==dups[i]])\n",
    "    \n",
    "    # if table run on all vars, the last gamma = 0,\n",
    "    #  instead set equal to the last pv_mono == final rate of unimp var inclusion\n",
    "    if(g_F[-1]==0): \n",
    "        g_F[-1]=pvs[-1]\n",
    "    \n",
    "    return g_F\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for model selection \"\"\"\n",
    "def alpha_F(g0, ncov, max_size=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "     Compute alpha-to-enter value corresponding to each step in model build procedure\n",
    "     \n",
    "    ### Input params:\n",
    "    #   g0       = float pre-specified FSR (gamma0)\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of alpha_F values\n",
    "    \"\"\"    \n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # alpha_F_i = gamma_0 * (1 + S_i) / (ncov - S_i)\n",
    "    alpha_F = g0 * (1 + S) / (ncov - S)\n",
    "    \n",
    "    # if table run on all vars, the last alpha = inf\n",
    "    #  instead set equal to 1 == include all vars\n",
    "    alpha_F[np.isinf(alpha_F)] = 1.\n",
    "    \n",
    "    return alpha_F        \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for specific gamma \"\"\"\n",
    "def alpha_F_g(g, gf, ncov):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "    # Compute alpha-to-enter for a pre-specified gamma (FSR)\n",
    "     \n",
    "    ### Input params:\n",
    "    #   g    = float or vector (length k) of specified FSR at which to compute alpha\n",
    "    #   gf   = vector gamma_F's computed from gamma0, pv_sorted\n",
    "            used to compute largest size model (S) for which gamma_F < g\n",
    "    #   ncov = integer of total number covariates in data\n",
    "    \n",
    "    ### Output:\n",
    "    # integer alpha_F value\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    ### Compute model size for gf closest to (but still <) g\n",
    "    #S = np.array([max(np.which(x<=y)) for x in gf y in g])+1\n",
    "    if isinstance(g,np.ndarray): # if g is a vector\n",
    "        s_s = [np.where(gf>y) for y in g]\n",
    "        S = np.array([min(x[0]) for x in s_s])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "    else: # if g is a number\n",
    "        S = min(np.where(gf>g)[0])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" Beta-hat computation for specific gamma \"\"\"\n",
    "def beta_est(x, y, g, gf, vname):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "    # Compute parameter estimates for final model given a pre-specified gamma (FSR)\n",
    "     \n",
    "    ### Input params:\n",
    "    #   x      = python dataframe of original p covariates, n x p\n",
    "    #   y      = python outcome dataframe, n x 1\n",
    "    #   g      = float of specified FSR at which to compute alpha\n",
    "    #   gf     = vector gamma_F's computed from gamma0, pv_mono\n",
    "                 used to compute largest size model (S) for which gamma_F < g\n",
    "    #   vname  = ordered vector of names of vars entered into model under forward selection\n",
    "    \n",
    "    ### Output:\n",
    "    # array of estimated parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    ### Compute model size corresponding to g\n",
    "    S = min(np.where(gf>g)[0])\n",
    "\n",
    "    ### Pull the cov names of those vars included in the above size model\n",
    "    modvars = vname[:S]\n",
    "\n",
    "    ### Fit the linear model using the selected model vars\n",
    "    fit = sm.OLS(y,x.loc[:,list(modvars)]).fit()\n",
    "    betaout = pd.DataFrame([fit.params,fit.bse]).T\n",
    "    betaout.columns = ['beta','beta_se']\n",
    "    \n",
    "    return betaout\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" FSR Results Table \"\"\"\n",
    "def fsrtable(size, vname, p_orig, p_mono, alphaf, gammaf, prec_f='.4f'):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "    # Build the results table for the ffsr function\n",
    "     \n",
    "    ### Input params:\n",
    "    #   size   = model size at each step of forward sel proc                   [S]\n",
    "    #   vname  = variable name that entered at each step (num vars = p)        [Var]\n",
    "    #   p_orig = p-values at each step                                         [p]\n",
    "    #   p_mono = ascending p-values                                            [p_s]\n",
    "    #   alphaf = alpha-to-enter (p-value cutoff) for model entry at each step  [alpha_F]\n",
    "    #   gammaf = FSR at each step                                              [gamma_F]\n",
    "    #   prec_f = string of precision (num digits) desired in FSR output table\n",
    "    \n",
    "    ### Output:\n",
    "    # table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Round all arrays\n",
    "    p_od = [format(x,prec_f) for x in p_orig]\n",
    "    p_md = [format(x,prec_f) for x in p_mono]\n",
    "    ad = [format(x,prec_f) for x in alphaf]\n",
    "    gd = [format(x,prec_f) for x in gammaf]\n",
    "    \n",
    "    ### Combine the arrays\n",
    "    tab = pd.DataFrame([size,vname,p_od,p_md,ad,gd]).T\n",
    "    tab.columns = ['S','Var','p','p_m','alpha_F','gamma_F']\n",
    "    \n",
    "    return tab\n",
    "\n",
    "\n",
    "\n",
    "# class ffsr_obj(object):\n",
    "#     if bag==False:\n",
    "#         if gs!=None: \n",
    "#             if betaout==True:\n",
    "#                 def __init__(self, fsr_results, betahats, gs):\n",
    "#                     self.fsres = fsr_results\n",
    "#                     self.beta = betahats\n",
    "#                     self.alpha = alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "#             else:\n",
    "#                 def __init__(self, fsr_results, gs):\n",
    "#                     self.fsres = fsr_results\n",
    "#                     self.alpha = alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "#         else:\n",
    "#             if betaout==True:\n",
    "#                 def __init__(self, fsr_results, betahats):\n",
    "#                     self.fsres = fsr_results\n",
    "#                     self.beta = betahats\n",
    "#             else:\n",
    "#                 def __init__(self, fsr_results):\n",
    "#                     self.fsres = fsr_results\n",
    "#     else:\n",
    "#         def __init__(self, betahats, g0):\n",
    "#             self.beta = betahats\n",
    "#             self.alpha = alpha_F_g(g0, g_F, d.shape[1]-1)\n",
    "#             self.size = len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR function \"\"\"\n",
    "def ffsr(dat,g0=0.05,betaout=False,gs=None,max_size=None,var_incl=None,bag=False,prec_f='.4f'):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "     Perform the Fast False Selection Rate procedure with linear regression\n",
    "     \n",
    "    ### Input params:\n",
    "    #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = string of precision (num digits) desired in FSR output table (string to be given to 'format' python fcn)\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    # Table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    #   S:       model size at given step\n",
    "    #   Var:     name of var that entered at given step\n",
    "    #   p:       p-value of var that entered at given step\n",
    "    #   p_m:     mono. inc. p-value (vector or original p-values arranged to be monotonically increasing)\n",
    "    #   alpha_F: cutoff value for model entry given gamma_0 and current p_s value\n",
    "    #   gamma_F: FSR given current alpha_F and model size (== step num)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure dat = pandas dataframes or else convert them\n",
    "    if bag==False:\n",
    "        if df_type(dat)==True:\n",
    "            if isinstance(dat,pd.DataFrame):\n",
    "                d = dat.copy()\n",
    "            else:\n",
    "                if isinstance(dat,np.ndarray):\n",
    "                    d = pd.DataFrame(dat)\n",
    "                    vnum = list(np.arange(d.shape[1])+1)\n",
    "                    vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                    d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "        else:\n",
    "            return df_type(dat)\n",
    "        \n",
    "        ### Remove missing values\n",
    "        d.dropna(inplace=True)\n",
    "        \n",
    "        ### Check that p < n to ensure regression solutions\n",
    "        if (d.shape[1]-1) >= d.shape[0]:\n",
    "            raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    else:\n",
    "        d = dat.copy()\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = d.shape[1]-1\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(d.columns.values[1:], max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, d.shape[1]-1, max_size)\n",
    "    \n",
    "    ### Check if betaout desired, if so compute beta_hat of model corresponding to specific gamma0\n",
    "    if betaout==True or bag==True:\n",
    "        betahats = beta_est(d.iloc[:,1:], pd.DataFrame(d.iloc[:,0]), g0, g_F, cov_entry_order)\n",
    "        \n",
    "    ### Check if bagging desired\n",
    "    if bag==False: \n",
    "        ### Alpha_F computation for all steps in fwd sel proc\n",
    "        a_F = alpha_F(g0, d.shape[1]-1, max_size)\n",
    "        \n",
    "        ### Model size\n",
    "        S = np.arange(max_size)+1\n",
    "        \n",
    "        ### Combine S, Cov_names, p-vals, sorted p-vals, alpha_F, gamma_F into table\n",
    "        fsr_results = fsrtable(S, cov_entry_order, p_orig, p_mono, a_F, g_F)\n",
    "        \n",
    "        ### Return selected output: FSR table (+ betahat) (+ alpha_specific)\n",
    "        if gs!=None: \n",
    "            ### Compute alpha_F for specific gammas (gs)\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "            else:\n",
    "                return fsr_results, alpha_F_g(gs, g_F, d.shape[1]-1)\n",
    "        else:\n",
    "            if betaout==True:\n",
    "                return fsr_results, betahats\n",
    "            else:\n",
    "                return fsr_results\n",
    "    else:\n",
    "        return betahats, alpha_F_g(g0, g_F, d.shape[1]-1), len(betahats)\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR for bagging function \"\"\"\n",
    "def ffsr_bag(dat,g0=0.05,max_size=None,var_incl=None,prec_f='.4f'):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "     Perform Fast False Selection Rate procedure in efficient manner conducive for bagging.\n",
    "     \n",
    "    ### NOTE: it is assumed that data has been transformed, cleaned, and is given in correct format.\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat      = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = string of precision (num digits) desired in FSR output table (string appropriate for 'format' python fcn)\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   number of param's in final model\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = dat.shape[1]-1\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(dat.iloc[:,1:], pd.DataFrame(dat.iloc[:,0]), max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(dat.columns.values[1:], max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, dat.shape[1]-1, max_size)\n",
    "    \n",
    "    ### Compute beta_hat of model corresponding to specific gamma0\n",
    "    betahats = beta_est(dat.iloc[:,1:], pd.DataFrame(dat.iloc[:,0]), g0, g_F, cov_entry_order)\n",
    "        \n",
    "    return betahats, alpha_F_g(g0, g_F, dat.shape[1]-1), len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "def bagfsr(dat,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Purpose:\n",
    "     Perform bagging with Fast False Selection Rate procedure to allow for more accurate predictions.\n",
    "     \n",
    "    ### Input params:\n",
    "    #   dat    = python dataframe of original p covariates, 1 outcome (in first col.): n x p+1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(dat)==True:\n",
    "        if isinstance(dat,pd.DataFrame):\n",
    "            d = dat.copy()\n",
    "        else:\n",
    "            if isinstance(dat,np.ndarray):\n",
    "                d = pd.DataFrame(dat)\n",
    "                vnum = list(np.arange(d.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",d.shape[1]))\n",
    "                d.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(dat)\n",
    "    \n",
    "    ### Remove missing values\n",
    "    d.dropna(inplace=True)\n",
    "    \n",
    "    ### check that p < n to ensure regression solutions\n",
    "    if (d.shape[1]-1) >= d.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(d.shape[1]-1),index=d.columns.values[1:])\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,(d.shape[1]-1)]),columns=d.columns.values[1:])\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "        # Draw with replacement from rows of data\n",
    "        newdat = pd.DataFrame(resample(d, replace=True))\n",
    "        newdat.columns = d.columns.values\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        fsrout = ffsr_bag(newdat,g0,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
    "        allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts of num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = allses.mean(axis=0)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize\n",
    "    \n",
    "    \n",
    "\n",
    "# Notes: \n",
    "# 1. appropriate transformations are expected to have been applied prior to utilization of FSR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
