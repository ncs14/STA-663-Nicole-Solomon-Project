{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%file ffsr.py\n",
    "\"\"\" Pseudocode for Fast FSR algorithm \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/8/15 \n",
    "    Modified: Profile primary function ffsr \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/17/15\n",
    "    Modified: Corrected p-value function, p-mono corrected \"\"\"\n",
    "\n",
    "\"\"\" Date: 4/19/15\n",
    "    Modified: Corrected gamma_F function to correctly handle duplicate p-values \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Data type check \"\"\"\n",
    "def df_type(dat):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   dat = dataset whose type is to be checked / transformed\n",
    "    \n",
    "    ### Output:\n",
    "    #   error msg or True boolean\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if isinstance(dat,pd.DataFrame)==False and isinstance(dat,np.ndarray)==False:\n",
    "        raise Exception(\"Data must be pandas DataFrame\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" p-value computation function \"\"\"\n",
    "def pval_comp(max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of p-values of each covariate at its given entry step\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.stats as st\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    # Pull RSS values & num_obs from fwd_proc object\n",
    "    rss = np.array(ro.r('fwd$rss'))\n",
    "    N = np.array(ro.r('fwd$nn'))\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(rss)-1\n",
    "    \n",
    "    # vector of model sizes\n",
    "    sizes = np.arange(max_size)+1\n",
    "    \n",
    "    # compute the F stats as defined above where p_f - p_r = 1 for each iteration\n",
    "    fstats = (rss[0:max_size] - rss[1:(max_size+1)]) / (rss[1:(max_size+1)] / (N - (sizes+1)))\n",
    "    \n",
    "    # return the p-values by comparing these stats to the F distn: F(1, n - p_f)\n",
    "    return 1 - st.f.cdf(fstats, 1, N-(sizes+1))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Covariate model entry order \"\"\"\n",
    "def cov_order(xcolnames,max_size=None,col_incl=None):\n",
    "    \n",
    "    # Input params:\n",
    "    #   xcolnames = array of names of covariates (same order as columns in original dataset)\n",
    "    #   max_size  = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl  = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # array of covariate names sorted according to order of entry into the model\n",
    "    \n",
    "    ### NOTE: fwd should be a global-env R object (requires running 'forward' fcn prior to this fcn) ###\n",
    "    \n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = len(xcolnames)\n",
    "        \n",
    "    ### Pull the cov entry order\n",
    "    vorder = ro.r('fwd$vorder[-1]') # remove intercept\n",
    "    vorder = vorder[0:max_size] # keep only the max model size number of covs\n",
    "    \n",
    "    ### Shift these values down by two (one to exclude intercept, one to make python indices)\n",
    "    vorderinds = np.array(vorder)-2\n",
    "    \n",
    "    ### Rearrange the var order st forced vars are at start of list\n",
    "    if col_incl==None:\n",
    "        col_incl = np.arange(max_size)+1\n",
    "    keep = xcolnames[[col_incl-1]] # pull var names of those vars forced into model (this is an array)\n",
    "    poss = [x for x in xcolnames if x not in keep] # pull var names of those not forced in (this is a list)\n",
    "    col_names = np.array(list(keep)+poss) # = rearranged array of varnames w/forced-in vars at start of list\n",
    "    \n",
    "    ### Sort the columns of X in order to obtain the var names in the entry order\n",
    "    return col_names[vorderinds[::]]\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Forward selection function \"\"\"\n",
    "def forward(x,y,max_size=None,col_incl=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    #   col_incl = array vector of columns to forcefully include in all models\n",
    "    \n",
    "    ### Output:\n",
    "    # regsubsets R object -- the raw full output of the forward selection proc\n",
    "    \n",
    "    ### Load python packages to call R functions\n",
    "    import rpy2.robjects as ro\n",
    "    import pandas.rpy.common as com\n",
    "    \n",
    "    ### Convert x and y to R matrices <-- MAKE SURE x,y input == DATAFRAMES (or else change them to df's)!!!\n",
    "    ### and declare as R objects in global environment\n",
    "    ro.globalenv['x2'] = com.convert_to_r_matrix(x)\n",
    "    ro.globalenv['y2'] = com.convert_to_r_matrix(y)\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "    ro.globalenv['maxv'] = ro.Vector(max_size)\n",
    "    if col_incl==None:\n",
    "        ro.r('coli=NULL')\n",
    "    else:\n",
    "        ro.globalenv['coli'] = ro.FloatVector(col_incl[:])\n",
    "    \n",
    "    ### Perform forward selection with regsubsets function\n",
    "    ro.globalenv['fwd'] = ro.r('leaps::regsubsets(x=x2,y=y2,method=\"forward\",nvmax=maxv,force.in=coli)')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Gamma computation \"\"\"\n",
    "def gamma_F(pvs, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   pvs      = vector of p-values (sorted or unsorted) from forward sel procedure\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of gamma_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # gamma_F_i = p_s_i * (ncov - S_i) / (1 + S_i)\n",
    "    g_F = pvs * (ncov - S) / (1 + S)\n",
    "    \n",
    "    # Check for duplicate p-values\n",
    "    dups = list(set([x for x in list(pvs) if list(pvs).count(x) > 1]))\n",
    "    for i in range(len(dups)): g_F[pvs==dups[i]] = min(g_F[pvs==dups[i]])\n",
    "    \n",
    "    # if table run on all vars, the last gamma = 0,\n",
    "    #  instead set equal to the last pv_sort == final rate of unimp var inclusion\n",
    "    if(g_F[-1]==0): \n",
    "        g_F[-1]=pvs[-1]\n",
    "    \n",
    "    return g_F\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for model selection \"\"\"\n",
    "def alpha_F(g0, ncov, max_size=None):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g0       = float pre-specified FSR (gamma0)\n",
    "    #   ncov     = integer total number of covariates in data\n",
    "    #   max_size = integer max no. of vars in final model (largest model size desired)\n",
    "    \n",
    "    ### Output:\n",
    "    # array of alpha_F values\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if max_size==None:\n",
    "        max_size = ncov\n",
    "        \n",
    "    # Create indices == model size at given step, call this S\n",
    "    S = np.arange(max_size)+1\n",
    "    \n",
    "    # alpha_F_i = gamma_0 * (1 + S_i) / (ncov - S_i)\n",
    "    alpha_F = g0 * (1 + S) / (ncov - S)\n",
    "    \n",
    "    # if table run on all vars, the last alpha = inf\n",
    "    #  instead set equal to 1 == include all vars\n",
    "    alpha_F[np.isinf(alpha_F)] = 1.\n",
    "    \n",
    "    return alpha_F        \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" Alpha computation for specific gamma \"\"\"\n",
    "def alpha_F_g(g, gf, ncov):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   g    = float or vector (length k) of specified FSR at which to compute alpha\n",
    "    #   gf   = vector gamma_F's computed from gamma0, pv_sorted\n",
    "    #          used to compute largest size model (S) for which gamma_F < g\n",
    "    #   ncov = integer of total number covariates in data\n",
    "    \n",
    "    ### Output:\n",
    "    # integer alpha_F value\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    ### Compute model size for gf closest to (but still <) g\n",
    "    #S = np.array([max(np.which(x<=y)) for x in gf y in g])+1\n",
    "    if isinstance(g,np.ndarray): # if g is a vector\n",
    "        s_s = [np.where(gf>y) for y in g]\n",
    "        S = np.array([min(x[0]) for x in s_s])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "    else: # if g is a number\n",
    "        S = min(np.where(gf>g)[0])\n",
    "        return g * (1 + S) / (ncov - S)\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" Beta-hat computation for specific gamma \"\"\"\n",
    "def beta_est(x, y, g, gf, vname):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x      = python dataframe of original p covariates, n x p\n",
    "    #   y      = python outcome dataframe, n x 1\n",
    "    #   g      = float of specified FSR at which to compute alpha\n",
    "    #   gf     = vector gamma_F's computed from gamma0, pv_sorted\n",
    "    #            used to compute largest size model (S) for which gamma_F < g\n",
    "    #   vname  = ordered vector of names of vars entered into model under forward selection\n",
    "    \n",
    "    ### Output:\n",
    "    # array of estimated parameters\n",
    "    \n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    ### Compute model size corresponding to g\n",
    "    S = min(np.where(gf>g)[0])\n",
    "\n",
    "    ### Pull the cov names of those vars included in the above size model\n",
    "    modvars = vname[:S]\n",
    "\n",
    "    ### Fit the linear model using the selected model vars\n",
    "    fit = sm.OLS(y,x.loc[:,list(modvars)]).fit()\n",
    "    betaout = pd.DataFrame([fit.params,fit.bse]).T\n",
    "    betaout.columns = ['beta','beta_se']\n",
    "    \n",
    "    return betaout\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" FSR Results Table \"\"\"\n",
    "def fsrtable(size, vname, p_orig, p_mono, alphaf, gammaf, prec_f=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   size   = model size at each step of forward sel proc                   [S]\n",
    "    #   vname  = variable name that entered at each step (num vars = p)        [Var]\n",
    "    #   p_orig = p-values at each step                                         [p]\n",
    "    #   p_mono = ascending p-values                                            [p_s]\n",
    "    #   alphaf = alpha-to-enter (p-value cutoff) for model entry at each step  [alpha_F]\n",
    "    #   gammaf = FSR at each step                                              [gamma_F]\n",
    "    #   prec_f   = integer of precision (num digits) desired in FSR output table\n",
    "    \n",
    "    ### Output:\n",
    "    # table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Convert all arrays to dataframes\n",
    "    sized = pd.DataFrame(size)\n",
    "    vnamed = pd.DataFrame(vname)\n",
    "    p_od = pd.DataFrame(np.around(p_orig,prec_f))\n",
    "    p_sd = pd.DataFrame(np.around(p_mono,prec_f))\n",
    "    ad = pd.DataFrame(np.around(alphaf,prec_f))\n",
    "    gd = pd.DataFrame(np.around(gammaf,prec_f))\n",
    "    \n",
    "    ### Combine the arrays\n",
    "    tab = pd.concat([sized,vnamed,p_od,p_sd,ad,gd],axis=1)\n",
    "    tab.columns = ['S','Var','p','p_s','alpha_F','gamma_F']\n",
    "    \n",
    "    return tab\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" FastFSR function \"\"\"\n",
    "def ffsr(X,Y,g0=0.05,betaout=False,gs=None,max_size=None,var_incl=None,bag=False,prec_f=4,prec_b=6):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = integer of precision (num digits) desired in FSR output table\n",
    "    #   prec_b   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    # Table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    #   S:       model size at given step\n",
    "    #   Var:     name of var that entered at given step\n",
    "    #   p:       p-value of var that entered at given step\n",
    "    #   p_s:     sorted p-value (vector or original p-values sorted in increasing order)\n",
    "    #   alpha_F: cutoff value for model entry given gamma_0 and current p_s value\n",
    "    #   gamma_F: FSR given current alpha_F and model size (== step num)\n",
    "    #       and\n",
    "    # Vector of alpha_F's for specified gamma's (g)\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(X)==True:\n",
    "        if isinstance(X,pd.DataFrame):\n",
    "            x = X.copy()\n",
    "        else:\n",
    "            if isinstance(X,np.ndarray):\n",
    "                x = pd.DataFrame(X)\n",
    "                vnum = list(np.arange(x.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",x.shape[1]))\n",
    "                x.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(X)\n",
    "    if df_type(Y)==True:\n",
    "        if isinstance(Y,pd.DataFrame):\n",
    "            y = Y.copy()\n",
    "        else:\n",
    "            if isinstance(Y,np.ndarray):\n",
    "                y = pd.DataFrame(Y)\n",
    "    else:\n",
    "        return df_type(Y)\n",
    "    \n",
    "    # remove missing values\n",
    "    yna = np.isnan(y).any(axis=1)\n",
    "    xna = np.isnan(x).any(axis=1).reshape(x.shape[0],1)\n",
    "    anyna = np.array([int(max(a,b)) for a,b in zip(xna,yna)])\n",
    "    missrow = np.where(anyna==1)[0]\n",
    "    y = y.drop(y.index[missrow])\n",
    "    x = x.drop(x.index[missrow])\n",
    "    \n",
    "    # check that p < n to ensure regression solutions\n",
    "    if x.shape[1] >= x.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(x, y, max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(x.columns.values, max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Sort p-values in ascending order\n",
    "    p_sort = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_sort, x.shape[1], max_size)\n",
    "    \n",
    "    ### Check if betaout desired, if so compute beta_hat of model corresponding to specific gamma0\n",
    "    if betaout==True or bag==True:\n",
    "        betahats = beta_est(x, y, g0, g_F, cov_entry_order)\n",
    "        \n",
    "    ### Check if bagging desired\n",
    "    if bag==False: \n",
    "        ### Alpha_F computation for all steps in fwd sel proc\n",
    "        a_F = alpha_F(g0, x.shape[1], max_size)\n",
    "        \n",
    "        ### Model size\n",
    "        S = np.arange(max_size)+1\n",
    "        \n",
    "        ### Combine S, Cov_names, p-vals, sorted p-vals, alpha_F, gamma_F into table\n",
    "        fsr_results = fsrtable(S, cov_entry_order, p_orig, p_sort, a_F, g_F)\n",
    "        \n",
    "        ### Return selected output: FSR table (+ betahat) (+ alpha_specific)\n",
    "        if gs!=None: \n",
    "            ### Compute alpha_F for specific gammas (gs)\n",
    "            if betaout==True:\n",
    "                return fsr_results, np.around(betahats, prec_b), alpha_F_g(gs, g_F, x.shape[1])\n",
    "            else:\n",
    "                return fsr_results, alpha_F_g(gs, g_F, x.shape[1])\n",
    "        else:\n",
    "            if betaout==True:\n",
    "                return fsr_results, np.around(betahats, prec_b)\n",
    "            else:\n",
    "                return fsr_results\n",
    "    else:\n",
    "        return betahats, alpha_F_g(g0, g_F, x.shape[1]), len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "def bagfsr(X,Y,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   X      = python dataframe of original p covariates, n x p\n",
    "    #   Y      = python outcome dataframe, n x 1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(X)==True:\n",
    "        if isinstance(X,pd.DataFrame):\n",
    "            x = X.copy()\n",
    "        else:\n",
    "            if isinstance(X,np.ndarray):\n",
    "                x = pd.DataFrame(X)\n",
    "                vnum = list(np.arange(x.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",x.shape[1]))\n",
    "                x.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(X)\n",
    "    if df_type(Y)==True:\n",
    "        if isinstance(Y,pd.DataFrame):\n",
    "            y = Y.copy()\n",
    "        else:\n",
    "            if isinstance(Y,np.ndarray):\n",
    "                y = pd.DataFrame(Y)\n",
    "    else:\n",
    "        return df_type(Y)\n",
    "    \n",
    "    ### Combine data into single dataframe\n",
    "    dat = pd.concat([y,x],axis=1)\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(x.shape[1]),index=x.columns.values)\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,x.shape[1]]),columns=x.columns.values)\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "\n",
    "        # Draw with replacement from rows of data\n",
    "        np.random.seed(1234)\n",
    "        n_row = dat.shape[0]\n",
    "        rand_row = np.random.randint(0,n_row,n_row)\n",
    "        newdat = dat.iloc[rand_row,:]\n",
    "        newdat.index = np.arange(n_row)+1\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        prec_all = 8 # precision of output\n",
    "        fsrout = ffsr(newdat.iloc[:,1:],pd.DataFrame(newdat.iloc[:,0]),g0,bag=True,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = np.around(fsrout[0].iloc[:,0],prec_all)\n",
    "        allses.loc[i,fsrout[0].index.values] = np.around(fsrout[0].iloc[:,1],prec_all)\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec_all))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = allbetas.mean(axis=0) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = allses.mean(axis=0)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize\n",
    "    \n",
    "    \n",
    "\n",
    "# Notes: \n",
    "# 1. appropriate transformations are expected to have been applied prior to utilization of FSR algorithm\n",
    "\n",
    "# To-do:\n",
    "# 1. adjust betaest fcn and ffsr to allow for specification of intercept and whether data should be normalized in estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "X = np.random.multivariate_normal(np.zeros(15),np.eye(15),(100))\n",
    "beta = np.array([0.,0,5,6,0,0,4,0,0,0,5,0,0,0,0]).reshape(15,1) # signif betas: 3,4,7,11\n",
    "Y = X.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y2 = pd.DataFrame(Y)\n",
    "X2 = pd.DataFrame(X)\n",
    "X2.columns = [\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_forw_select(features, max_k, criterion_func, print_steps=False):\n",
    "    \"\"\"\n",
    "    Implementation of a Sequential Forward Selection algorithm.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        features (list): The feature space as a list of features.\n",
    "        max_k: Termination criterion; the size of the returned feature subset.\n",
    "        criterion_func (function): Function that is used to evaluate the\n",
    "            performance of the feature subset.\n",
    "        print_steps (bool): Prints the algorithm procedure if True.\n",
    "    \n",
    "    Returns the selected feature subset, a list of features of length max_k.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialization\n",
    "    feat_sub = []\n",
    "    k = 0\n",
    "    d = len(features)\n",
    "    if max_k > d:\n",
    "        max_k = d\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Inclusion step\n",
    "        if print_steps:\n",
    "            print('\\nInclusion from feature space', features)\n",
    "        crit_func_max = criterion_func(feat_sub + [features[0]])\n",
    "        best_feat = features[0]\n",
    "        for x in features[1:]:\n",
    "            crit_func_eval = criterion_func(feat_sub + [x])\n",
    "            if crit_func_eval > crit_func_max:\n",
    "                crit_func_max = crit_func_eval\n",
    "                best_feat = x\n",
    "        feat_sub.append(best_feat)\n",
    "        if print_steps:\n",
    "            print('include: {} -> feature subset: {}'.format(best_feat, feat_sub))\n",
    "        features.remove(best_feat)\n",
    "        \n",
    "        # Termination condition\n",
    "        k = len(feat_sub)\n",
    "        if k == max_k:\n",
    "            break\n",
    "                \n",
    "    return feat_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Comparisons with R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading required package: leaps\n",
       "   user  system elapsed \n",
       "  0.614   0.007   0.635 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i X2,Y2\n",
    "\n",
    "bag.fsr<-function(x,y,B=100,gam0=.05){\n",
    "# gives average coefficients from fsr.fast6.sim\n",
    "ok<-complete.cases(x,y)\n",
    "x<-x[ok,]                            # get rid of na's\n",
    "y<-y[ok]                             # since regsubsets can't handle na's\n",
    "m<-ncol(x)\n",
    "n<-nrow(x)\n",
    "hold<-matrix(rep(0,m*B),nrow=B)      # holds coefficients\n",
    "interc<-rep(0,B)                     # holds intercepts\n",
    "alphahat<-rep(0,B)                   # holds alphahats\n",
    "size<-rep(0,B)                       # holds sizes\n",
    "for(i in 1:B){\n",
    "index<-sample(1:n,n,replace=T)\n",
    "out<-fsr.fast6.sim(x=x[index,],y=y[index],gam0=gam0)\n",
    "if (out$size>0) hold[i,out$x.ind]<-out$mod$coeff[2:(out$size+1)]\n",
    "interc[i]<-out$mod$coeff[1]\n",
    "alphahat[i]<-out$alphahat.ER\n",
    "size[i]<-out$size\n",
    "}                                    # ends i loop\n",
    "coeff.av<-apply(hold,2, mean)\n",
    "coeff.sd<-rep(0,m)\n",
    "coeff.sd<-sqrt(apply(hold,2, var))\n",
    "interc.av<-mean(interc)\n",
    "interc.sd<-sd(interc)\n",
    "amean<-mean(alphahat)\n",
    "sizem<-mean(size)\n",
    "prop<-rep(0,m)\n",
    "for(j in 1:m){prop[j]<-sum(abs(hold[,j])>0)/B}\n",
    "as.matrix(x)->x                      # in case x is a data frame\n",
    "pred<-x%*%coeff.av+interc.av\n",
    "return(list(coeff.av=coeff.av,coeff.sd=coeff.sd,interc.av=interc.av,pred=pred,\n",
    "            interc.sd=interc.sd,prop=prop,amean=amean,sizem=sizem))\n",
    "}\n",
    "\n",
    "fsr.fast6.sim<-function(x,y,gam0=.05){\n",
    "# estimated alpha for forward selection\n",
    "# short output version\n",
    "require(leaps)\n",
    "ok<-complete.cases(x,y)\n",
    "x<-x[ok,]                            # get rid of na's\n",
    "y<-y[ok]                             # since regsubsets can't handle na's\n",
    "m<-ncol(x)\n",
    "n<-nrow(x)\n",
    "if(m >= n) m1 <- n-5  else m1<-m     # to get rid of NA's in pv\n",
    "vm<-1:m1\n",
    "as.matrix(x)->x                      # in case x is a data frame\n",
    "pvm<-rep(0,m1)                       # to create pvm below\n",
    "regsubsets(x,y,method=\"forward\")->out.x\n",
    "pv.orig<-1-pf((out.x$rss[vm]-out.x$rss[vm+1])*(out.x$nn-(vm+1))/out.x$rss[vm+1],1,out.x$nn-(vm+1))\n",
    "for (i in 1:m1){pvm[i]<-max(pv.orig[1:i])}  # sequential max of pvalues\n",
    "alpha<-c(0,pvm)\n",
    "ng<-length(alpha)\n",
    "S<-rep(0,ng)                         # will contain num. of true entering in orig.\n",
    "real.seq<-data.frame(var=(out.x$vorder-1)[2:(m1+1)],pval=pv.orig,\n",
    "         pvmax=pvm,Rsq=round(1-out.x$rss[2:(m1+1)]/out.x$rss[1],4))\n",
    "for (ia in 2:ng){                    # loop through alpha values for S=size\n",
    "S[ia] <- sum(pvm<=alpha[ia])         # size of models at alpha[ia], S[1]=0\n",
    "}\n",
    "ghat<-(m-S)*alpha/(1+S)              # gammahat_ER\n",
    "# add additional points to make jumps\n",
    "alpha2<-alpha[2:ng]-.0000001\n",
    "ghat2<-(m-S[1:(ng-1)])*alpha2/(1+S[1:(ng-1)])\n",
    "zp<-data.frame(a=c(alpha,alpha2),g=c(ghat,ghat2))\n",
    "zp<-zp[order(zp$a),]\n",
    "gmax<-max(zp$g)\n",
    "index.max<-which.max(zp$g)           # index of largest ghat\n",
    "alphamax<-zp$a[index.max]            # alpha with largest ghat\n",
    "ind<-(ghat <= gam0 & alpha<=alphamax)*1\n",
    "Sind<-S[max(which(ind > 0))]          # model size with ghat just below gam0\n",
    "alphahat.fast<-(1+Sind)*gam0/(m-Sind)  # ER est.\n",
    "size1<-sum(pvm<=alphahat.fast)+1       # size of model including intercept\n",
    "colnames(x)<-colnames(x,do.NULL=F,prefix=\"\")      # corrects for no colnames\n",
    "x<-x[,colnames(x)[(out.x$vorder-1)[2:size1]]]\n",
    "if(size1>1) x.ind<-(out.x$vorder-1)[2:size1]  else x.ind<-0\n",
    "if (size1==1) {mod <- lm(y~1)} else {mod <- lm(y~x)}\n",
    "return(list(mod=mod,size=size1-1,x.ind=x.ind,alphahat.ER=alphahat.fast))\n",
    "}\n",
    "    \n",
    "x2 = as.matrix(X2)\n",
    "y2 = as.matrix(Y2)\n",
    "\n",
    "system.time(bag.fsr(x=x2,y=y2,B=200)->outt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.73 s, sys: 81.7 ms, total: 4.82 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "b = bagfsr(X2,Y2,0.05) # <-- TOO SLOW! see future drafts for edits to bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FastFSR function \"\"\"\n",
    "def ffsr(X,Y,g0=0.05,betaout=False,gs=None,max_size=None,var_incl=None,bag=False,prec_f=4,prec_b=6):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   x        = python dataframe of original p covariates, n x p\n",
    "    #   y        = python outcome dataframe, n x 1\n",
    "    #   g0       = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   betaout  = boolean of whether to include estimated betahats from final selected model\n",
    "    #   gs       = float or vector of gamma's at which to specifically compute alpha_F\n",
    "    #   max_size = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   var_incl = array of cols corresponding to those vars to force into model\n",
    "    #   bag      = boolean of whether to output FSR table (non-bagging results) or reduced output for bagging purposes\n",
    "    #   prec_f   = integer of precision (num digits) desired in FSR output table\n",
    "    #   prec_b   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #      (note: gamma = FSR, gamma_0 = pre-specified/desired FSR)\n",
    "    # Table of [S   Var   p   p_s   alpha_F   gamma_F], dim = num_steps(== p) x 6\n",
    "    #   S:       model size at given step\n",
    "    #   Var:     name of var that entered at given step\n",
    "    #   p:       p-value of var that entered at given step\n",
    "    #   p_m:     mono. inc. p-value (vector or original p-values arranged to be monotonically increasing)\n",
    "    #   alpha_F: cutoff value for model entry given gamma_0 and current p_s value\n",
    "    #   gamma_F: FSR given current alpha_F and model size (== step num)\n",
    "    #       and\n",
    "    #   vector of alpha_F's for specified gamma's (g)\n",
    "    #       and\n",
    "    #   vector of estimated beta param's for final model (based on g0)\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if bag==False:\n",
    "        if df_type(X)==True:\n",
    "            if isinstance(X,pd.DataFrame):\n",
    "                x = X.copy()\n",
    "            else:\n",
    "                if isinstance(X,np.ndarray):\n",
    "                    x = pd.DataFrame(X)\n",
    "                    vnum = list(np.arange(x.shape[1])+1)\n",
    "                    vchr = list(np.repeat(\"V\",x.shape[1]))\n",
    "                    x.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "        else:\n",
    "            return df_type(X)\n",
    "        if df_type(Y)==True:\n",
    "            if isinstance(Y,pd.DataFrame):\n",
    "                y = Y.copy()\n",
    "            else:\n",
    "                if isinstance(Y,np.ndarray):\n",
    "                    y = pd.DataFrame(Y)\n",
    "        else:\n",
    "            return df_type(Y)\n",
    "        # Remove missing values\n",
    "        yna = np.isnan(y).any(axis=1)\n",
    "        xna = np.isnan(x).any(axis=1).reshape(x.shape[0],1)\n",
    "        anyna = np.array([int(max(a,b)) for a,b in zip(xna,yna)])\n",
    "        missrow = np.where(anyna==1)[0]\n",
    "        y = y.drop(y.index[missrow])\n",
    "        x = x.drop(x.index[missrow])\n",
    "        # Check that p < n to ensure regression solutions\n",
    "        if x.shape[1] >= x.shape[0]:\n",
    "            raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    else:\n",
    "        x, y = X.copy(), Y.copy()\n",
    "    \n",
    "    ### If max model size not specified, select all possible cov.s\n",
    "    if max_size==None:\n",
    "        max_size = x.shape[1]\n",
    "        \n",
    "    ### Perform forward selection\n",
    "    fwd_sel = forward(x, y, max_size, var_incl)\n",
    "    \n",
    "    ### Save order of covariate entry into model\n",
    "    cov_entry_order = cov_order(x.columns.values, max_size, var_incl)\n",
    "    \n",
    "    ### Compute p-value of each covariate entering the model\n",
    "    p_orig = pval_comp(max_size)\n",
    "    \n",
    "    ### Arrange p-values in mono. inc. order\n",
    "    p_mono = np.array([max(p_orig[:(i+1)]) for i in range(len(p_orig))])\n",
    "        \n",
    "    ### Gamma_F computation\n",
    "    g_F = gamma_F(p_mono, x.shape[1], max_size)\n",
    "    \n",
    "    ### Check if betaout desired, if so compute beta_hat of model corresponding to specific gamma0\n",
    "    if betaout==True or bag==True:\n",
    "        betahats = beta_est(x, y, g0, g_F, cov_entry_order)\n",
    "        \n",
    "    ### Check if bagging desired\n",
    "    if bag==False: \n",
    "        ### Alpha_F computation for all steps in fwd sel proc\n",
    "        a_F = alpha_F(g0, x.shape[1], max_size)\n",
    "        \n",
    "        ### Model size\n",
    "        S = np.arange(max_size)+1\n",
    "        \n",
    "        ### Combine S, Cov_names, p-vals, sorted p-vals, alpha_F, gamma_F into table\n",
    "        fsr_results = fsrtable(S, cov_entry_order, p_orig, p_mono, a_F, g_F)\n",
    "        \n",
    "        ### Return selected output: FSR table (+ betahat) (+ alpha_specific)\n",
    "        if gs!=None: \n",
    "            ### Compute alpha_F for specific gammas (gs)\n",
    "            if betaout==True:\n",
    "                return fsr_results, np.around(betahats, prec_b), alpha_F_g(gs, g_F, x.shape[1])\n",
    "            else:\n",
    "                return fsr_results, alpha_F_g(gs, g_F, x.shape[1])\n",
    "        else:\n",
    "            if betaout==True:\n",
    "                return fsr_results, np.around(betahats, prec_b)\n",
    "            else:\n",
    "                return fsr_results\n",
    "    else:\n",
    "        return betahats, alpha_F_g(g0, g_F, x.shape[1]), len(betahats)\n",
    "\n",
    "    \n",
    "    \n",
    "def bagfsr(X,Y,g0,B=200,max_s=None,v_incl=None,prec=4):\n",
    "    \n",
    "    ### Input params:\n",
    "    #   X      = python dataframe of original p covariates, n x p\n",
    "    #   Y      = python outcome dataframe, n x 1\n",
    "    #   g0     = float pre-specified FSR of interest (\"gamma0\")\n",
    "    #   B      = integer of number of bagged samples\n",
    "    #   max_s  = integer of largest model size == max num vars to incl in final model (default = num covs in dataset)\n",
    "    #   v_incl = array of cols corresponding to those vars to force into model\n",
    "    #   prec   = integer of precision (num digits) desired in beta-hat parameter estimates of final model\n",
    "    \n",
    "    ### Output: \n",
    "    #   Mean of betahats\n",
    "    #   SEs of betahats\n",
    "    #   Avg alpha-to-enter\n",
    "    #   Avg model size\n",
    "    #   Prop of times each var included in model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Clean and check data - make sure X, Y = pandas dataframes or else convert them\n",
    "    if df_type(X)==True:\n",
    "        if isinstance(X,pd.DataFrame):\n",
    "            x = X.copy()\n",
    "        else:\n",
    "            if isinstance(X,np.ndarray):\n",
    "                x = pd.DataFrame(X)\n",
    "                vnum = list(np.arange(x.shape[1])+1)\n",
    "                vchr = list(np.repeat(\"V\",x.shape[1]))\n",
    "                x.columns = [a + str(b) for a,b in zip(vchr,vnum)]\n",
    "    else:\n",
    "        return df_type(X)\n",
    "    if df_type(Y)==True:\n",
    "        if isinstance(Y,pd.DataFrame):\n",
    "            y = Y.copy()\n",
    "        else:\n",
    "            if isinstance(Y,np.ndarray):\n",
    "                y = pd.DataFrame(Y)\n",
    "    else:\n",
    "        return df_type(Y)\n",
    "    \n",
    "    # Remove missing values\n",
    "    yna = np.isnan(y).any(axis=1)\n",
    "    xna = np.isnan(x).any(axis=1).reshape(x.shape[0],1)\n",
    "    anyna = np.array([int(max(a,b)) for a,b in zip(xna,yna)])\n",
    "    missrow = np.where(anyna==1)[0]\n",
    "    y = y.drop(y.index[missrow])\n",
    "    x = x.drop(x.index[missrow])\n",
    "    \n",
    "    # check that p < n to ensure regression solutions\n",
    "    if x.shape[1] >= x.shape[0]:\n",
    "        raise Exception(\"N must be > p for valid regression solutions\")\n",
    "    \n",
    "    ### Combine data into single dataframe\n",
    "    dat = pd.concat([y,x],axis=1)\n",
    "    \n",
    "    ### Create array to keep track of number of times vars enter model\n",
    "    nentries = pd.DataFrame(np.zeros(x.shape[1]),index=x.columns.values)\n",
    "    \n",
    "    ### Create array to store all estimated coefficients, ses, alphas, sizes\n",
    "    allbetas = pd.DataFrame(np.zeros([B,x.shape[1]]),columns=x.columns.values)\n",
    "    allses = allbetas.copy()\n",
    "    alphas = []\n",
    "    sizes = []\n",
    "    \n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    ### Bagging loops\n",
    "    for i in range(B):\n",
    "\n",
    "        # Draw with replacement from rows of data\n",
    "        n_row = dat.shape[0]\n",
    "        rand_row = np.random.randint(0,n_row,n_row)\n",
    "        newdat = dat.iloc[rand_row,:]\n",
    "        newdat.index = np.arange(n_row)+1\n",
    "        \n",
    "        ### Obtain FSR results\n",
    "        fsrout = ffsr(newdat.iloc[:,1:],pd.DataFrame(newdat.iloc[:,0]),g0,bag=True,max_size=max_s,var_incl=v_incl)\n",
    "        allbetas.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,0]\n",
    "        allses.loc[i,fsrout[0].index.values] = fsrout[0].iloc[:,1]\n",
    "        alphas.append(fsrout[1])\n",
    "        sizes.append(fsrout[2])\n",
    "\n",
    "        ### Update counts num times var included\n",
    "        nentries.loc[fsrout[0].index[np.abs(np.around(fsrout[0].iloc[:,0],prec))>0]] += 1\n",
    "        \n",
    "    ### Compute averages\n",
    "    avgbeta = np.around(allbetas.mean(axis=0),prec) # mean across rows / colmeans == mean of each cov's betahat\n",
    "    avgse = np.around(allses.mean(axis=0),prec)\n",
    "    avgalpha = np.mean(alphas)\n",
    "    avgsize = np.mean(sizes)\n",
    "    var_props = nentries/float(B)\n",
    "    cov_res = pd.concat([avgbeta,avgse,var_props],axis=1)\n",
    "    cov_res.columns = ['betahat','betase','prop_incl']\n",
    "    \n",
    "    return cov_res, avgalpha, avgsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.33 s, sys: 70.2 ms, total: 4.4 s\n",
      "Wall time: 4.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     betahat  betase  prop_incl\n",
       " V1         0       0          0\n",
       " V2         0       0          0\n",
       " V3         5       0          1\n",
       " V4         6       0          1\n",
       " V5         0       0          0\n",
       " V6        -0       0          0\n",
       " V7         4       0          1\n",
       " V8         0       0          0\n",
       " V9        -0       0          0\n",
       " V10        0       0          0\n",
       " V11        5       0          1\n",
       " V12        0       0          0\n",
       " V13        0       0          0\n",
       " V14       -0       0          0\n",
       " V15       -0       0          0, 0.049823376623376624, 6.1449999999999996)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "bagfsr(X2,Y2,0.05) # <-- TOO SLOW! see future drafts for edits to bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparing the above R result with %%time result of my algorithm shows the following increase:\n",
    "# user: + 4.12s,  sys: + 7.07ms,  elapsed: + 4.192ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# OPTIMIZATION PLAN:\n",
    "\n",
    "\"\"\"  \n",
    "NOTE: numba, numbapro do not work on (any) some of my FSR functions due to incompatibility w/list comprehension (among other things)\n",
    "note to self: write subfunction to remove missing values and attempt following optimization strategies\n",
    "\n",
    "1. Try to vectorize additional lines of code to improve speed/efficiency\n",
    "2. Try to translate some functions into C-code (perhaps using numexpr), or else find c equivalents which would be faster than the \n",
    "   given python versions\n",
    "    a. This should be viable for those functions that do not manipulate R objects\n",
    "    b. Perhaps search for a C/C++ forward selection function (likely faster than regsubsets which I am calling from R)\n",
    "3. Closely examine code to determine where memory is needlessly being used (e.g. are duplicates being made unnecessarily)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
